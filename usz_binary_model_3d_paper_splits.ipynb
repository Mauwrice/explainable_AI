{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Binary Model based on 3D Image Data and the Paper Splits\n",
    "\n",
    "The model is based on the following paper https://arxiv.org/abs/2206.13302. However, only binary classification is performed.  \n",
    "The same splits as in the paper are used. To be able to compare the new splits with the paper splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (0.12.2)\n",
      "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (0.5.3)\n",
      "Requirement already satisfied: pandas>=0.21 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (1.1.5)\n",
      "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (1.19.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21->statsmodels) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21->statsmodels) (2021.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.5->statsmodels) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.19.4)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.1.5)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.5.4)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.3.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (8.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->seaborn) (2021.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF  Version 2.4.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from scipy import ndimage\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "print(\"TF  Version\",tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf\n",
      "TF  Version 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# check and set path before loading modules\n",
    "print(os.getcwd())\n",
    "DIR = \"/tf/notebooks/brdd/xAI_stroke_3d/\"\n",
    "if os.getcwd() != DIR:\n",
    "    os.chdir(DIR)\n",
    "\n",
    "import functions_metrics as fm\n",
    "import functions_model_definition as md\n",
    "import functions_read_data as rdat\n",
    "\n",
    "print(\"TF  Version\",tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.experimental.get_memory_usage(\"GPU:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path + output path:\n",
    "print(os.getcwd())\n",
    "IMG_DIR = \"/tf/notebooks/hezo/stroke_zurich/data/\" \n",
    "# IMG_DIR2 = \"/tf/notebooks/kook/data-sets/stroke-lh/\"\n",
    "DATA_DIR = \"/tf/notebooks/hezo/stroke_zurich/data/\" \n",
    "OUTPUT_DIR = \"/tf/notebooks/brdd/xAI_stroke_3d/weights/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split and model name have to be defined manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_split = 1\n",
    "# model_name = \"3d_cnn_binary_model_split6_unnormalized_flatten_layer_paper_model_{epoch:03d}.h5\"\n",
    "model_name = \"3d_cnn_binary_model_split\" + str(which_split) + \"_unnormalized_avg_layer_paper_model_sigmoid_activation_14.h5\"\n",
    "train= False\n",
    "\n",
    "layer_connection = \"globalAveragePooling\"\n",
    "last_activation = \"sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_valid, X_test), (y_train, y_valid, y_test), results = rdat.read_and_split_img_data(\n",
    "    path_img = IMG_DIR + 'dicom_2d_192x192x3_clean_interpolated_18_02_2021_preprocessed2.h5', \n",
    "    path_tab = IMG_DIR + 'baseline_data_zurich_prepared.csv', \n",
    "    path_splits = '/tf/notebooks/brdd/xAI_stroke_3d/data/andrea_splits.csv', \n",
    "    split = which_split, \n",
    "    check_print = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(y_train))\n",
    "print(np.mean(y_valid))\n",
    "print(np.mean(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #augmentation functions\n",
    "\n",
    "# @tf.function\n",
    "# def rotate(volume):\n",
    "#     \"\"\"Rotate the volume by a few degrees\"\"\"\n",
    "\n",
    "#     def scipy_rotate(volume):\n",
    "#         # define some rotation angles\n",
    "#         angles = [-20, -10, -5, 5, 10, 20]\n",
    "#         # pick angles at random\n",
    "#         angle = random.choice(angles)\n",
    "#         # rotate volume\n",
    "#         volume = ndimage.rotate(volume, angle, mode='nearest', reshape=False)\n",
    "# #         volume[volume < 0] = 0\n",
    "# #         volume[volume > 1] = 1\n",
    "#         return volume\n",
    "\n",
    "#     augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n",
    "#     return augmented_volume\n",
    "\n",
    "\n",
    "# @tf.function\n",
    "# def zoom(volume):\n",
    "#     \"\"\"Rotate the volume by a few degrees\"\"\"\n",
    "\n",
    "#     def random_zoom3d(X_im, min_zoom = 0.85 , max_zoom = 1.15):\n",
    "#         z = np.random.sample() *(max_zoom-min_zoom) + min_zoom\n",
    "#         zoom_matrix = np.array([[z, 0, 0, 0],\n",
    "#                             [0, z, 0, 0],\n",
    "#                             [0, 0, z, 0],\n",
    "#                             [0, 0, 0, 1]])\n",
    "#         volume = ndimage.affine_transform(X_im, zoom_matrix, mode = \"nearest\", order = 1)\n",
    "#         return volume\n",
    "\n",
    "#     augmented_volume = tf.numpy_function(random_zoom3d, [volume], tf.float32)\n",
    "#     return augmented_volume\n",
    "\n",
    "\n",
    "# @tf.function\n",
    "# def gauss(volume):\n",
    "#     \"\"\"Add some noise and smooth it with a gaussian filter\"\"\"\n",
    "\n",
    "#     def random_gaussianfilter3d(X_im, sigma_max=0.2):\n",
    "#         sigma = np.random.uniform(0, sigma_max)\n",
    "#         volume = ndimage.gaussian_filter(X_im, sigma, mode = \"nearest\")\n",
    "#         return volume\n",
    "\n",
    "#     augmented_volume = tf.numpy_function(random_gaussianfilter3d, [volume], tf.float32)\n",
    "#     return augmented_volume\n",
    "\n",
    "\n",
    "# def train_preprocessing(volume, label):\n",
    "#     \"\"\"Process training data by rotating and adding a channel.\"\"\"\n",
    "#     # Rotate volume\n",
    "#     volume = rotate(volume)\n",
    "#     volume = zoom(volume)\n",
    "# #     volume = gauss(volume)\n",
    "#     volume = tf.expand_dims(volume, axis=3)\n",
    "#     return volume, label\n",
    "\n",
    "\n",
    "def validation_preprocessing(volume, label):\n",
    "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = np.expand_dims(X_train, axis = -1).shape[1:]\n",
    "output_dim = 1\n",
    "\n",
    "# call model\n",
    "model_3d = md.stroke_binary_3d(input_dim = input_dim,\n",
    "                               output_dim = output_dim,\n",
    "                               layer_connection = layer_connection,\n",
    "                               last_activation = last_activation)\n",
    "model_3d.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders.\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "\n",
    "batch_size = 6\n",
    "\n",
    "# Augment on the fly during training.\n",
    "# train_dataset = (\n",
    "#     train_loader.shuffle(len(X_train))\n",
    "#     .map(train_preprocessing)\n",
    "#     .batch(batch_size)\n",
    "#     .prefetch(2)\n",
    "# )\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    fill_mode=\"nearest\")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "validation_dataset = (\n",
    "    validation_loader.shuffle(len(X_valid))\n",
    "    .map(validation_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "\n",
    "#compile\n",
    "model_3d.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=5*1e-5),\n",
    "    metrics=[\"acc\", tf.keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "# Define callbacks.\n",
    "# checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "#     OUTPUT_DIR + model_name, \n",
    "#     save_best_only=True\n",
    "# )\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    filepath = OUTPUT_DIR + \"epochs/\" + model_name,\n",
    "    verbose = 1,\n",
    "    save_weights_only = True,\n",
    "    monitor = \"val_loss\", #'val_acc',\n",
    "    mode = 'min',\n",
    "    save_best_only = True)\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=100, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Augemntation\n",
    "# data = train_dataset.take(1)\n",
    "# images, labels = list(data)[0]\n",
    "# images = images.numpy()\n",
    "data = datagen.flow(X_train, y_train, batch_size=batch_size)[0]\n",
    "images, labels = data\n",
    "image = images[0]\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "for i in range(image.shape[2]):\n",
    "    plt.subplot(10,7,i+1)\n",
    "    plt.imshow(np.squeeze(image[:, :, i]), cmap=\"gray\", vmin = np.min(image), vmax = np.max(image))\n",
    "#     plt.imshow(np.squeeze(image[:, i,: ]), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.title(str(\"Aug. slice: \" + str(i+1)))\n",
    "plt.show()\n",
    "\n",
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, doing validation at the end of each epoch\n",
    "epochs = 250\n",
    "\n",
    "if train:\n",
    "    hist = model_3d.fit(\n",
    "        datagen.flow(X_train, y_train, batch_size=batch_size, shuffle=True),\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=epochs,\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        callbacks=[checkpoint_cb, early_stopping_cb]\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    plt.figure(figsize = (30,10))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(hist.history['loss'], label = \"loss\")\n",
    "    plt.plot(hist.history['val_loss'],label = \"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(hist.history['acc'], label = \"acc\")\n",
    "    plt.plot(hist.history['val_acc'],label = \"val_acc\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(hist.history['auc'], label = \"auc\")\n",
    "    plt.plot(hist.history['val_auc'],label = \"val_auc\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    epoch_index = 55\n",
    "    print(hist.history[\"val_loss\"][epoch_index])\n",
    "    print(hist.history[\"val_acc\"][epoch_index])\n",
    "    print(hist.history[\"val_auc\"][epoch_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train:\n",
    "    model_3d.load_weights(\"weights/\" + model_name)\n",
    "if train:\n",
    "    model_3d.load_weights(\"weights/epochs/\" + model_name)\n",
    "model_3d.evaluate(x=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_3d.predict(X_test)\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = fm.sigmoid(y_pred)\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_3d.load_weights(\"weights/\" + model_name)\n",
    "fm.bin_class_report(X_test, \n",
    "                 y_test, \n",
    "                 model = model_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_3d.predict(X_test)\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, (y_pred))\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# method I: plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmean = np.sqrt(tpr * (1 - fpr))\n",
    "\n",
    "# Find the optimal threshold\n",
    "index = np.argmax(gmean)\n",
    "print(threshold[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc\n",
    "y_pred_label = (y_pred >= threshold[index]).squeeze()\n",
    "np.mean(y_pred_label == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc to beat\n",
    "1 - np.mean(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"spezifität: \", 1-fpr[index])\n",
    "print(\"sensitivität: \", tpr[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"pred_prob\"] = y_pred.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(x = \"unfavorable\",\n",
    "            y = \"pred_prob\",\n",
    "            data = results)\n",
    "sns.stripplot(x = \"unfavorable\",\n",
    "              y = \"pred_prob\",\n",
    "              color = 'blue',\n",
    "              data = results)\n",
    "plt.axhline(y = threshold[index], color = \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Plots and Result Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_3d.predict(X_test)\n",
    "y_pred = y_pred.squeeze()\n",
    "print(np.min(y_pred), np.max(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_plot_data = fm.cal_plot_data_prep(y_pred, y_test)\n",
    "cal_plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.cal_plot(cal_plot_data, \"predicted_probability_middle\", \"observed_proportion\",\n",
    "                        \"observed_proportion_lower\", \"observed_proportion_upper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Andreas Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "andrea_result_path = \"/tf/notebooks/kook/dtm-usz-stroke/intermediate-results/\"\n",
    "andrea_results_ens1 = pd.read_csv(\n",
    "    andrea_result_path + 'raw-results/stroke_cimrsbinary_lossnll_wsno_augyes_cdftest_spl' + str(which_split) + '_ens1.csv'\n",
    "    ).rename(columns={\"Unnamed: 0\": \"p_idx\"})\n",
    "andrea_results_ens2 = pd.read_csv(\n",
    "    andrea_result_path + 'raw-results/stroke_cimrsbinary_lossnll_wsno_augyes_cdftest_spl' + str(which_split) + '_ens2.csv'\n",
    "    ).rename(columns={\"Unnamed: 0\": \"p_idx\"})\n",
    "andrea_results_ens3 = pd.read_csv(\n",
    "    andrea_result_path + 'raw-results/stroke_cimrsbinary_lossnll_wsno_augyes_cdftest_spl' + str(which_split) + '_ens3.csv'\n",
    "    ).rename(columns={\"Unnamed: 0\": \"p_idx\"})\n",
    "andrea_results_ens4 = pd.read_csv(\n",
    "    andrea_result_path + 'raw-results/stroke_cimrsbinary_lossnll_wsno_augyes_cdftest_spl' + str(which_split) + '_ens4.csv'\n",
    "    ).rename(columns={\"Unnamed: 0\": \"p_idx\"})\n",
    "andrea_results_ens5 = pd.read_csv(\n",
    "    andrea_result_path + 'raw-results/stroke_cimrsbinary_lossnll_wsno_augyes_cdftest_spl' + str(which_split) + '_ens5.csv'\n",
    "    ).rename(columns={\"Unnamed: 0\": \"p_idx\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"andrea_pred_prob_ens1\"] = 1-andrea_results_ens1[\"V2\"]\n",
    "results[\"andrea_pred_prob_ens2\"] = 1-andrea_results_ens2[\"V2\"]\n",
    "results[\"andrea_pred_prob_ens3\"] = 1-andrea_results_ens2[\"V2\"]\n",
    "results[\"andrea_pred_prob_ens4\"] = 1-andrea_results_ens2[\"V2\"]\n",
    "results[\"andrea_pred_prob_ens5\"] = 1-andrea_results_ens2[\"V2\"]\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.scatterplot(data=results, x=\"pred_prob\", y=\"andrea_pred_prob_ens1\", hue = \"unfavorable\")\n",
    "plt.legend(loc='lower right')\n",
    "g.set(ylim=(0, 1), xlim=(0,1))\n",
    "g.plot([0,1], [0,1], \"r--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calipration Plot: all of Andrea vs. current split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# andrea_calplot_dat = pd.read_csv('/tf/notebooks/brdd/xAI_stroke_3d/data/bincal_avgnll.csv')\n",
    "andrea_calplot_dat = pd.read_csv(andrea_result_path + 'bincal_avgnll.csv')\n",
    "andrea_calplot_cibinary_avg = andrea_calplot_dat[(andrea_calplot_dat[\"mod\"] == \"cimrsbinary\") &\n",
    "                                                 (andrea_calplot_dat[\"method\"] == \"trafo\") &\n",
    "                                                 (andrea_calplot_dat[\"weights\"] == \"equal\")]\n",
    "andrea_calplot_cibinary_avg\n",
    "# cal_plot(andrea_calplot_cibinary_avg, \"midpoint\", \"prop\", \"lwr\", \"upr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "andrea_calplot_dat_spl = pd.read_csv('/tf/notebooks/brdd/xAI_stroke_3d/data/bincal_splnll.csv')\n",
    "andrea_calplot_cibinary_spl = andrea_calplot_dat_spl[(andrea_calplot_dat_spl[\"mod\"] == \"cimrsbinary\") &\n",
    "                                                     (andrea_calplot_dat_spl[\"method\"] == \"trafo\") &\n",
    "                                                     (andrea_calplot_dat_spl[\"weights\"] == \"equal\")]\n",
    "andrea_calplot_cibinary_spl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(4, 4))\n",
    "\n",
    "for i in range(5):\n",
    "    fm.cal_plot(andrea_calplot_cibinary_spl[andrea_calplot_cibinary_spl[\"spl\"] == i], \n",
    "             \"midpoint\", \"prop\", \"lwr\", \"upr\", alpha = .35, show = False)\n",
    "fm.cal_plot(andrea_calplot_cibinary_avg, \"midpoint\", \"prop\", \"lwr\", \"upr\", show = False)\n",
    "fm.cal_plot(cal_plot_data, \"predicted_probability_middle\", \"observed_proportion\",\n",
    "                        \"observed_proportion_lower\", \"observed_proportion_upper\", col = \"orange\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
