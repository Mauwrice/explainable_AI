{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF  Version 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "import pickle as pkl\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "print(\"TF  Version\",tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and set path before loading modules\n",
    "INPUT_DIR = \"/tf/notebooks/schnemau/xAI_stroke_3d/\"\n",
    "OUTPUT_DIR = \"/tf/notebooks/schnemau/xAI_stroke_3d/\"\n",
    "if os.getcwd() != OUTPUT_DIR:\n",
    "    os.chdir(OUTPUT_DIR)\n",
    "    \n",
    "import functions_model_definition as md\n",
    "import functions_read_data as rdat\n",
    "from functions.augmentation3d import zoom, rotate, flip, shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Version\n",
    "version = \"CIBLSX\" # one of:\n",
    "#version = \"CIB\" # one of:\n",
    "\n",
    "# Define Model Version\n",
    "model_version = 6\n",
    "\n",
    "# Select naming convention (for CIBLSX model_version >= 3 should be False)\n",
    "comp_mode = False # if True: use old naming convention\n",
    "\n",
    "# define paths\n",
    "DATA_DIR, WEIGHT_DIR, DATA_OUTPUT_DIR, PIC_OUTPUT_DIR, pic_save_name = rdat.dir_setup(\n",
    "    INPUT_DIR, OUTPUT_DIR, version, model_version, \n",
    "    compatibility_mode=comp_mode)\n",
    "\n",
    "save_csv = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Table does not exist for CIBLSX M6. Returning None for all_results_tab.\n"
     ]
    }
   ],
   "source": [
    "## load images and ids\n",
    "(X_in, pat_ids, id_tab, all_results_tab, pat_orig_tab, pat_norm_tab, num_models) = rdat.version_setup(\n",
    "    DATA_DIR = DATA_DIR, \n",
    "    version = version, \n",
    "    model_version = model_version,\n",
    "    compatibility_mode=comp_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "(input_dim_img, output_dim, LOSS, layer_connection, last_activation) = md.model_setup(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Name\n",
    "generate_model_name = md.set_generate_model_name(\n",
    "    model_version = model_version, \n",
    "    layer_connection = layer_connection, \n",
    "    last_activation = last_activation, \n",
    "    path = WEIGHT_DIR,\n",
    "    compatability_mode=comp_mode)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nrs = list(range(num_models)) #num of ensembles\n",
    "which_splits = list(range(0,10)) # 10 Fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for augmentation\n",
    "if pat_norm_tab is not None:\n",
    "    def train_preprocessing(data, label):\n",
    "        volume = data[0]\n",
    "        tabular = data[1]\n",
    "        volume = zoom(volume)\n",
    "        volume = rotate(volume)\n",
    "        volume = shift(volume)\n",
    "        volume = flip(volume)\n",
    "        return (volume, tabular), label\n",
    "else: \n",
    "    def train_preprocessing(data, label):\n",
    "        volume = data  \n",
    "        volume = zoom(volume)\n",
    "        volume = rotate(volume)\n",
    "        volume = shift(volume)\n",
    "        volume = flip(volume)\n",
    "        return (volume), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "################################################################################\n",
      "Split 0\n",
      "################################################################################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/500\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.3066 - acc: 0.8642\n",
      "Epoch 00001: val_loss improved from inf to 0.41738, saving model to /tf/notebooks/schnemau/xAI_stroke_3d/weights/10Fold_CIBLSX/3D_CNN_avg_layer_binary_outcome_CIBLSX_split0_ens0_M6.h5\n",
      "54/54 [==============================] - 42s 774ms/step - loss: 0.3066 - acc: 0.8642 - val_loss: 0.4174 - val_acc: 0.9167\n",
      "Epoch 2/500\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.3154 - acc: 0.8704\n",
      "Epoch 00002: val_loss did not improve from 0.41738\n",
      "54/54 [==============================] - 42s 781ms/step - loss: 0.3154 - acc: 0.8704 - val_loss: 0.4269 - val_acc: 0.9167\n",
      "Epoch 3/500\n",
      "37/54 [===================>..........] - ETA: 13s - loss: 0.3299 - acc: 0.8559"
     ]
    }
   ],
   "source": [
    "start0 = time.time()\n",
    "for which_split in which_splits:\n",
    "    start1 = time.time()\n",
    "    print(\"\\n\\n\\n\\n################################################################################\")\n",
    "    print(\"Split \" + str(which_split))\n",
    "    print(\"################################################################################\\n\\n\\n\\n\")\n",
    "    \n",
    "    data_split = rdat.split_data(id_tab, X_in, which_split, X_tab = pat_norm_tab)\n",
    "    \n",
    "    #Images\n",
    "    X_valid = np.expand_dims(data_split[\"X\"][\"valid\"], axis=-1)\n",
    "    X_train = np.expand_dims(data_split[\"X\"][\"train\"], axis=-1)\n",
    "    \n",
    "    #Outcomes    \n",
    "    Y_valid = to_categorical(data_split[\"y\"][\"valid\"])\n",
    "    Y_train = to_categorical(data_split[\"y\"][\"train\"])\n",
    "\n",
    "      \n",
    "    #Tabular data\n",
    "\n",
    "    if pat_norm_tab is not None:\n",
    "        X_tab_train = data_split[\"X_tab\"][\"train\"]    \n",
    "        X_tab_valid = data_split[\"X_tab\"][\"valid\"]\n",
    "        train_data = tf.data.Dataset.from_tensor_slices((X_train, X_tab_train))\n",
    "        valid_data = tf.data.Dataset.from_tensor_slices((X_valid, X_tab_valid))\n",
    "    else:\n",
    "        X_tab_test = None\n",
    "        X_tab_valid = None\n",
    "        train_data = tf.data.Dataset.from_tensor_slices((X_train))\n",
    "        valid_data = tf.data.Dataset.from_tensor_slices((X_valid))\n",
    "\n",
    "    \n",
    "    valid_labels = tf.data.Dataset.from_tensor_slices((Y_valid))\n",
    "    valid_loader = tf.data.Dataset.zip((valid_data, valid_labels))\n",
    "    valid_dataset = (valid_loader.batch(6, drop_remainder = True)) \n",
    "\n",
    "    train_labels = tf.data.Dataset.from_tensor_slices((Y_train))\n",
    "    train_loader = tf.data.Dataset.zip((train_data, train_labels))\n",
    "    train_dataset = (train_loader.shuffle(len(X_train)).map(train_preprocessing).batch(6, drop_remainder=True))\n",
    "\n",
    "    #Logistic regression to set_weights initialization\n",
    "    \n",
    "    model_noshrink = LogisticRegression(penalty='none')\n",
    "    model_noshrink.fit(data_split[\"X_tab\"][\"train\"] , data_split[\"y\"][\"train\"])\n",
    "    model_shrink = LogisticRegression()\n",
    "    model_shrink.fit(data_split[\"X_tab\"][\"train\"] , data_split[\"y\"][\"train\"])\n",
    "    average_logreg_weights = (model_noshrink.coef_.T + model_shrink.coef_.T)/2\n",
    "\n",
    "    for model_nr in model_nrs:\n",
    "\n",
    "        cnn_weight_path = os.path.join(INPUT_DIR, \"weights/10Fold_CIB\", f\"3D_CNN_avg_layer_binary_outcome_CIB_split{which_split}_ens{model_nr}_M2.h5\")      \n",
    "        \n",
    "        start2 = time.time()\n",
    "\n",
    "        model_3d = md.model_init(\n",
    "            version = version, \n",
    "            output_dim = output_dim,\n",
    "            LOSS = LOSS,\n",
    "            layer_connection = layer_connection,\n",
    "            last_activation = last_activation,\n",
    "            C = 2,\n",
    "            learning_rate = 5*1e-5,\n",
    "            batch_size = 6,\n",
    "            input_dim = input_dim_img,\n",
    "            input_dim_tab = pat_norm_tab.drop(columns=[\"p_id\"]).shape[1] if \"LSX\" in version else None,\n",
    "            weights_tab_init = average_logreg_weights, \n",
    "            cnn_weights_init_path = cnn_weight_path) \n",
    "        \n",
    "        if pat_norm_tab is not None:\n",
    "            model_name = (\"3D_CNN_avg_layer_binary_outcome_CIBLSX_split\" + str(which_split) + \n",
    "                      \"_ens\" + str(model_nr) + \"_M\" + str(model_version) + \".h5\")\n",
    "        else:\n",
    "            model_name = (\"3D_CNN_avg_layer_binary_outcome_CIB_split\" + str(which_split) + \n",
    "                      \"_ens\" + str(model_nr) + \"_M\" + str(model_version) + \".h5\")\n",
    "\n",
    "        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "            filepath = WEIGHT_DIR + model_name,\n",
    "            verbose = (1 if which_split == 0 and model_nr == 0 else 0),\n",
    "            save_weights_only = True,\n",
    "            monitor = \"val_loss\", \n",
    "            mode = 'min',\n",
    "            save_best_only = True)\n",
    "\n",
    "        early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=60, restore_best_weights=True)\n",
    "               \n",
    "        hist = model_3d.fit(\n",
    "            train_dataset,\n",
    "            validation_data=valid_dataset,\n",
    "            epochs=500,\n",
    "            shuffle=True,\n",
    "            verbose=(1 if which_split == 0 and model_nr == 0 else 0), \n",
    "            callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "        pkl.dump(hist.history, open(WEIGHT_DIR + \"hist_\" + model_name[:-2] + \"pkl\", \"wb\"), protocol=4)     \n",
    "        \n",
    "        end2 = time.time()\n",
    "        print(\" \")   \n",
    "        print(\"Duration of Training: \" + str(end2-start2))  \n",
    "        \n",
    "    end1 = time.time()\n",
    "    print(\" \")   \n",
    "    print(\"Duration of Split: \" + str(end1-start1))  \n",
    "        \n",
    "end0 = time.time()\n",
    "print(\" \")\n",
    "print(\"Duration of Everything: \" + str(end0-start0))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
