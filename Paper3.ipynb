{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maurice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-in, prepare data & load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install seaborn\n",
    "#!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle as pkl\n",
    "\n",
    "#import functions_read_data as rdat\n",
    "# Tensorflow/Keras\n",
    "import tensorflow as tf\n",
    "import time\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pickle as pkl\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/notebooks/schnemau/xAI_stroke_3d\n"
     ]
    }
   ],
   "source": [
    "DIR = \"/tf/notebooks/schnemau/xAI_stroke_3d/\"\n",
    "os.chdir(DIR)\n",
    "print(os.getcwd())\n",
    "IMG_DIR = \"/tf/notebooks/hezo/stroke_perfusion/data/\"\n",
    "OUTPUT_DIR = \"/tf/notebooks/schnemau/xAI_stroke_3d/data/\"\n",
    "path_img = IMG_DIR + 'dicom_2d_192x192x3_clean_interpolated_18_02_2021_preprocessed2.h5'\n",
    "path_tab = IMG_DIR + 'baseline_data_zurich_prepared.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_metrics as fm\n",
    "import functions_model_definition as md\n",
    "import functions_read_data as rdat\n",
    "import Utils_maurice as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own functions\n",
    "from functions.plot_slices import plot_slices\n",
    "#ontram functions\n",
    "from k_ontram_functions.ontram import ontram\n",
    "from k_ontram_functions.ontram_loss import ontram_loss\n",
    "from k_ontram_functions.ontram_metrics import ontram_acc, ontram_auc\n",
    "from k_ontram_functions.ontram_predict import predict_ontram, get_parameters\n",
    "from functions.augmentation3d import zoom, rotate, flip, shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_split = 6\n",
    "(X_train, X_valid, X_test, X_tab_train, X_tab_valid, X_tab_test), (Y_train, Y_valid, Y_test), results = utils.read_and_split_img_data_andrea_maurice(\n",
    "    path_img = IMG_DIR + 'dicom_2d_192x192x3_clean_interpolated_18_02_2021_preprocessed2.h5', \n",
    "    path_tab = IMG_DIR + 'baseline_data_zurich_prepared.csv', \n",
    "    path_splits = '/tf/notebooks/schnemau/xAI_stroke_3d/data/andrea_splits.csv', \n",
    "    split = which_split)\n",
    "\n",
    "Y_train_MRS = to_categorical(Y_train)\n",
    "Y_valid_MRS = to_categorical(Y_valid)\n",
    "Y_test_MRS = to_categorical(Y_test)\n",
    "\n",
    "#X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3], 1))\n",
    "#X_valid = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], X_valid.shape[2], X_valid.shape[3], 1))\n",
    "#X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], X_test.shape[3], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONTRAM-Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jonas-Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stroke_binary_3d_softmax(input_dim = (128, 128, 28,1), \n",
    "                     output_dim = 2,\n",
    "                     last_activation = \"softmax\"):\n",
    "    \n",
    "    initializer = keras.initializers.he_normal(seed = 2202)\n",
    "    \n",
    "    #input\n",
    "    inputs = keras.Input(input_dim)\n",
    "    \n",
    "    # conv block 0\n",
    "    x = layers.Convolution3D(32, kernel_size=(3, 3, 3), padding = 'same', activation = 'relu')(inputs)\n",
    "    x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    \n",
    "    # conv block 1\n",
    "    x = layers.Convolution3D(32, kernel_size=(3, 3, 3), padding = 'same', activation = 'relu')(x)\n",
    "    x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    \n",
    "    # conv block 2\n",
    "    x = layers.Convolution3D(64, kernel_size=(3, 3, 3), padding = 'same', activation = 'relu')(x)\n",
    "    x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    \n",
    "    # conv block 3\n",
    "    x = layers.Convolution3D(64, kernel_size=(3, 3, 3), padding = 'same',activation = 'relu')(x)\n",
    "    x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    \n",
    "    #x = layers.Flatten()(x)\n",
    "    x = layers.GlobalAveragePooling3D()(x) \n",
    "    # flat block\n",
    "    x = layers.Dense(128, activation = 'relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(128, activation = 'relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    out = layers.Dense(units=output_dim, activation = last_activation)(x) # softmax (output_dim must be at least 2)\n",
    "\n",
    "    # Define the model.\n",
    "    model_3d_softmax = Model(inputs=inputs, outputs=out)\n",
    "    \n",
    "    return model_3d_softmax\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Prep for Training, Valid, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = Y_train_MRS.shape[1]\n",
    "batch_size = 5\n",
    "epochs = 250\n",
    "ntrain = 325\n",
    "nvalid = 40\n",
    "ntest = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CI_B Real no ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_preprocessing(volume, label):\n",
    "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((X_train[:ntrain], Y_train_MRS[:ntrain]))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((X_valid[:nvalid], Y_train_MRS[:nvalid]))\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    fill_mode=\"nearest\")\n",
    "datagen.fit(X_train[:ntrain])\n",
    "\n",
    "validation_dataset = (\n",
    "    validation_loader.shuffle(len(X_valid))\n",
    "    .map(validation_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = np.expand_dims(X_train, axis = -1).shape[1:]\n",
    "output_dim = 1\n",
    "\n",
    "# call model\n",
    "ontram_CI_B_real = ontram(img_model_linear(input_dim, output_dim))\n",
    "\n",
    "#compile\n",
    "ontram_CI_B_real.compile(optimizer=keras.optimizers.Adam(learning_rate=5*1e-5),\n",
    "                        loss=ontram_loss(C, batch_size),\n",
    "                        metrics=[ontram_acc(C, batch_size)])\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    filepath = '/tf/notebooks/schnemau/xAI_stroke_3d/ensembling_results/real2.h5',\n",
    "    verbose = 1,\n",
    "    save_weights_only = True,\n",
    "    monitor = \"val_loss\", #'val_acc',\n",
    "    mode = 'min',\n",
    "    save_best_only = True)\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=100, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CIB_real = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, doing validation at the end of each epoch\n",
    "epochs = 1\n",
    "\n",
    "if train_CIB_real:\n",
    "    hist = ontram_CI_B_real.fit(\n",
    "        datagen.flow(X_train, Y_train_MRS, batch_size=batch_size, shuffle=True),\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=epochs,\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        callbacks=[checkpoint_cb, early_stopping_cb]\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CI_B no Real ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_preprocessing(volume, label):\n",
    "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tab_ens = pd.read_csv(\"/tf/notebooks/schnemau/xAI_stroke_3d/data/10Fold_ids_V0.csv\", sep=\",\") # for V0\n",
    "X_ens = np.load(\"/tf/notebooks/schnemau/xAI_stroke_3d/data/prepocessed_dicom_3d.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 10   # 10 for all models\n",
    "num_models = 5    # see overview of all models (above)\n",
    "\n",
    "batch_size = 5    \n",
    "epochs = 250      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CIB_noreal = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start0 = time.time()\n",
    "for i in range(num_splits):\n",
    "    start1 = time.time()\n",
    "    print(\"\\n\\n\\n\\n################################################################################\")\n",
    "    print(\"Split \" + str(i))\n",
    "    print(\"################################################################################\\n\\n\\n\\n\")\n",
    "    \n",
    "    (X_train, X_valid, X_test), (y_train, y_valid, y_test) = rdat.split_data(id_tab_ens, X_ens, i)\n",
    "    \n",
    "    y_train_enc = to_categorical(y_train)\n",
    "    y_valid_enc = to_categorical(y_valid)\n",
    "    y_test_enc = to_categorical(y_test)\n",
    "\n",
    "    # loop over model instances (ensembling)\n",
    "    for j in range(num_models):\n",
    "        start2 = time.time()\n",
    "        print(\"\\n\\n#######################################################\")\n",
    "        print(\"Split \" + str(i) + \" Model \" + str(j))\n",
    "        print(\"#######################################################\\n\\n\")       \n",
    "\n",
    "        # call model\n",
    "        input_dim = np.expand_dims(X_train, axis = -1).shape[1:]\n",
    "        output_dim = 1\n",
    "        model_3d = ontram(img_model_linear(input_dim, output_dim))         \n",
    "       \n",
    "        # Define data loaders.\n",
    "        \n",
    "        train_loader = tf.data.Dataset.from_tensor_slices((X_train[:ntrain], y_train_enc[:ntrain]))\n",
    "        validation_loader = tf.data.Dataset.from_tensor_slices((X_valid[:nvalid], y_valid_enc[:nvalid]))\n",
    "\n",
    "        # data augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            zoom_range=0.15,\n",
    "            shear_range=0.15,\n",
    "            fill_mode=\"nearest\")\n",
    "        datagen.fit(X_train[:ntrain])\n",
    "\n",
    "        validation_dataset = (\n",
    "            validation_loader.shuffle(len(X_valid[:nvalid]))\n",
    "            .map(validation_preprocessing)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(2)\n",
    "        )\n",
    "\n",
    "\n",
    "        #compile\n",
    "        model_3d.compile(optimizer=keras.optimizers.Adam(learning_rate=5*1e-5),\n",
    "                                loss=ontram_loss(C, batch_size),\n",
    "                                metrics=[ontram_acc(C, batch_size)])      \n",
    "        \n",
    "        \n",
    "        \n",
    "        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "            filepath = os.path.join('/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/', f'model_ontram{j}_CIB_{i}.h5'),\n",
    "            verbose = (1 if i == 0 and j == 0 else 0),\n",
    "            save_weights_only = True,\n",
    "            monitor = \"val_loss\", #'val_acc',\n",
    "            mode = 'min',\n",
    "            save_best_only = True)\n",
    "\n",
    "        early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=100, restore_best_weights=True)\n",
    "        \n",
    "        # Train the model, doing validation at the end of each epoch\n",
    "        if train_CIB_noreal:\n",
    "            hist = model_3d.fit(\n",
    "                datagen.flow(X_train[:ntrain], y_train_enc[:ntrain], batch_size=batch_size, shuffle=True),\n",
    "                validation_data=validation_dataset,\n",
    "                epochs=epochs,\n",
    "                shuffle=True,\n",
    "                verbose=(1 if i == 0 and j == 0 else 0),\n",
    "                callbacks=[checkpoint_cb, early_stopping_cb]\n",
    "            ) \n",
    "            pkl.dump(hist.history, open(os.path.join('/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/', f'model_{j}_ontram_CIB_{i}.pkl')), protocol=4)\n",
    "            histplt = hist.history         \n",
    "       \n",
    "        \n",
    "        end2 = time.time()\n",
    "        print(\" \")   \n",
    "        print(\"Duration of Training: \" + str(end2-start2))  \n",
    "        \n",
    "    end1 = time.time()\n",
    "    print(\" \")   \n",
    "    print(\"Duration of Split: \" + str(end1-start1))  \n",
    "        \n",
    "end0 = time.time()\n",
    "print(\" \")\n",
    "print(\"Duration of Everything: \" + str(end0-start0))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CI_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = Y_train_MRS.shape[1]\n",
    "batch_size = 5\n",
    "epochs = 250\n",
    "ntrain = 325\n",
    "nvalid = 40\n",
    "ntest = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for preprocessing\n",
    "def train_preprocessing(data, label):\n",
    "    \"\"\"Process training data.\"\"\"\n",
    "    volume = data\n",
    "    volume = zoom(volume)\n",
    "    volume = rotate(volume)\n",
    "    volume = shift(volume)\n",
    "    volume = flip(volume)\n",
    "    return volume, label\n",
    "\n",
    "# Define data sets for Jonas Model - no ordinal structure of 0,1\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((X_train[:ntrain], Y_train[:ntrain]))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((X_valid[:nvalid], Y_valid[:nvalid]))\n",
    "test_loader = tf.data.Dataset.from_tensor_slices((X_test[:ntest], Y_test[:ntest]))\n",
    "\n",
    "train_dataset = (train_loader.shuffle(len(X_train[:ntrain]))\n",
    "                 .map(train_preprocessing)\n",
    "                 .batch(batch_size, drop_remainder = True))\n",
    "\n",
    "validation_dataset = (validation_loader.batch(batch_size, drop_remainder = True))\n",
    "test_dataset = (test_loader.batch(len(X_test[:ntest])))\n",
    "\n",
    "# Define data sets MRS for ordinal structure\n",
    "train_loader_MRS = tf.data.Dataset.from_tensor_slices((X_train[:ntrain], Y_train_MRS[:ntrain]))\n",
    "validation_loader_MRS = tf.data.Dataset.from_tensor_slices((X_valid[:nvalid], Y_valid_MRS[:nvalid]))\n",
    "test_loader_MRS = tf.data.Dataset.from_tensor_slices((X_test[:ntest], Y_test_MRS[:ntest]))\n",
    "\n",
    "train_dataset_MRS = (train_loader_MRS.shuffle(len(X_train[:ntrain]))\n",
    "                 .map(train_preprocessing)\n",
    "                 .batch(batch_size, drop_remainder = True))\n",
    "\n",
    "validation_dataset_MRS = (validation_loader_MRS.batch(batch_size, drop_remainder = True))\n",
    "test_dataset_MRS = (test_loader_MRS.batch(len(X_test[:ntest])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_CIB = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fit_CIB:\n",
    "    for i in range(5):\n",
    "            mod_ontram_lin2 = ontram(img_model_linear(X_train.shape[1:], 1))\n",
    "            mod_ontram_lin2.compile(optimizer=keras.optimizers.Adam(5*1e-5),\n",
    "                                    loss=ontram_loss(C, batch_size),\n",
    "                                    metrics=[ontram_acc(C, batch_size)])\n",
    "\n",
    "            checkpoint = ModelCheckpoint(os.path.join('/tf/notebooks/schnemau/xAI_stroke_3d/ensembling_results/', f'model_{i}_ontram_CIB_fini.h5'),\n",
    "                                        save_best_only=True, save_weights_only=True)\n",
    "            #early_stopping = EarlyStopping(patience=8, restore_best_weights=True)\n",
    "\n",
    "            mod_ontram_lin2.fit(train_dataset_MRS,\n",
    "                                validation_data=validation_dataset_MRS,\n",
    "                                epochs=250,\n",
    "                                shuffle=True,\n",
    "                                callbacks=checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CI_B COMPLETE JONAS SET-UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = Y_train_MRS.shape[1]\n",
    "batch_size = 5\n",
    "epochs = 250\n",
    "ntrain = 325\n",
    "nvalid = 40\n",
    "ntest = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_model_linear_final(input_shape, output_shape, activation = \"linear\"):\n",
    "    initializer = keras.initializers.he_normal(seed = 2202)\n",
    "    in_ = keras.Input(shape = input_shape)\n",
    "\n",
    "    # conv block 0\n",
    "    x = keras.layers.Convolution3D(32, kernel_size=(3, 3, 3), padding = 'same', activation = 'relu')(in_)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    # conv block 1\n",
    "    x = keras.layers.Convolution3D(32, kernel_size=(3, 3, 3), padding = 'same', activation = 'relu')(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    # conv block 2\n",
    "    x = keras.layers.Convolution3D(64, kernel_size=(3, 3, 3), padding = 'same', activation = 'relu')(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    # conv block 3\n",
    "    x = keras.layers.Convolution3D(64, kernel_size=(3, 3, 3), padding = 'same', activation = 'relu')(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "\n",
    "    # cnn to flat connection\n",
    "    x = layers.GlobalAveragePooling3D()(x) \n",
    "    \n",
    "    # flat block\n",
    "    x = keras.layers.Dense(128, activation = 'relu')(x)\n",
    "    x = keras.layers.Dropout(0.3)(x)\n",
    "    x = keras.layers.Dense(128, activation = 'relu')(x)\n",
    "    x = keras.layers.Dropout(0.3)(x)\n",
    "    out_ = keras.layers.Dense(output_shape, activation = activation, use_bias = False)(x) \n",
    "    nn_im = keras.Model(inputs = in_, outputs = out_)\n",
    "    return nn_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_preprocessing(volume, label):\n",
    "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call model\n",
    "input_dim = np.expand_dims(X_train, axis = -1).shape[1:]\n",
    "#input_dim = np.expand_dims(X_train, axis = -1).shape[1:5]\n",
    "output_dim = C-1\n",
    "model_3d = ontram(img_model_linear_final(input_dim, output_dim))         \n",
    "\n",
    "# Define data loaders.\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((X_train[:ntrain], Y_train_MRS[:ntrain]))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((X_valid[:nvalid], Y_valid_MRS[:nvalid]))\n",
    "\n",
    "# data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    fill_mode=\"nearest\")\n",
    "datagen.fit(X_train[:ntrain])\n",
    "\n",
    "validation_dataset = (\n",
    "    validation_loader.shuffle(len(X_valid[:nvalid]))\n",
    "    .map(validation_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "\n",
    "#compile\n",
    "model_3d.compile(optimizer=keras.optimizers.Adam(learning_rate=5*1e-5),\n",
    "                        loss=ontram_loss(C, batch_size),\n",
    "                        metrics=[ontram_acc(C, batch_size)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CIB_noreal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3d.fit(\n",
    "        datagen.flow(X_train[:ntrain], Y_train_MRS[:ntrain], batch_size=batch_size, shuffle=True),\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=250,\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        steps_per_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    filepath = os.path.join('/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/', f'iniCIB_1.h5'),\n",
    "    verbose = 1,\n",
    "    save_weights_only = True,\n",
    "    monitor = \"val_loss\", #'val_acc',\n",
    "    mode = 'min',\n",
    "    save_best_only = True)\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=100, restore_best_weights=True)\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch\n",
    "if train_CIB_noreal:\n",
    "    hist = model_3d.fit(\n",
    "        datagen.flow(X_train[:ntrain], Y_train_MRS[:ntrain], batch_size=batch_size, shuffle=True),\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=250,\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        steps_per_epoch = 150,        \n",
    "        callbacks=[checkpoint_cb, early_stopping_cb]) \n",
    "    pkl.dump(hist.history, open(os.path.join('/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/', f'iniCIB_1.pkl')), protocol=4)\n",
    "    histplt = hist.history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:940: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (325, 128, 128, 28) (28 channels).\n",
      "  ' channels).')\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py:127: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (325, 128, 128, 28) (28 channels).\n",
      "  str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.5055 - acc: 0.8040\n",
      "Epoch 00001: val_loss improved from inf to 0.46902, saving model to /tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/CIB_0_.h5\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.5055 - acc: 0.8040 - val_loss: 0.4690 - val_acc: 0.8250\n",
      "Epoch 2/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.4908 - acc: 0.8100\n",
      "Epoch 00002: val_loss improved from 0.46902 to 0.46834, saving model to /tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/CIB_0_.h5\n",
      "1000/1000 [==============================] - 114s 114ms/step - loss: 0.4908 - acc: 0.8100 - val_loss: 0.4683 - val_acc: 0.8250\n",
      "Epoch 3/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.4796 - acc: 0.8098\n",
      "Epoch 00003: val_loss did not improve from 0.46834\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.4796 - acc: 0.8098 - val_loss: 0.4719 - val_acc: 0.8250\n",
      "Epoch 4/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.4553 - acc: 0.8088\n",
      "Epoch 00004: val_loss improved from 0.46834 to 0.45438, saving model to /tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/CIB_0_.h5\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.4553 - acc: 0.8088 - val_loss: 0.4544 - val_acc: 0.8250\n",
      "Epoch 5/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.4282 - acc: 0.8164\n",
      "Epoch 00005: val_loss improved from 0.45438 to 0.43672, saving model to /tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/CIB_0_.h5\n",
      "1000/1000 [==============================] - 114s 114ms/step - loss: 0.4282 - acc: 0.8164 - val_loss: 0.4367 - val_acc: 0.8250\n",
      "Epoch 6/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.3911 - acc: 0.8434\n",
      "Epoch 00006: val_loss did not improve from 0.43672\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.3911 - acc: 0.8434 - val_loss: 0.4423 - val_acc: 0.8250\n",
      "Epoch 7/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.3681 - acc: 0.8548\n",
      "Epoch 00007: val_loss improved from 0.43672 to 0.43137, saving model to /tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/CIB_0_.h5\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.3681 - acc: 0.8548 - val_loss: 0.4314 - val_acc: 0.8250\n",
      "Epoch 8/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.3364 - acc: 0.8690\n",
      "Epoch 00008: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.3364 - acc: 0.8690 - val_loss: 0.5048 - val_acc: 0.8000\n",
      "Epoch 9/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.3066 - acc: 0.8872\n",
      "Epoch 00009: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.3066 - acc: 0.8872 - val_loss: 0.4487 - val_acc: 0.8500\n",
      "Epoch 10/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.2789 - acc: 0.8968\n",
      "Epoch 00010: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.2789 - acc: 0.8968 - val_loss: 0.4936 - val_acc: 0.8000\n",
      "Epoch 11/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.2520 - acc: 0.9070\n",
      "Epoch 00011: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.2520 - acc: 0.9070 - val_loss: 0.5061 - val_acc: 0.7750\n",
      "Epoch 12/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.2173 - acc: 0.9192\n",
      "Epoch 00012: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.2173 - acc: 0.9192 - val_loss: 0.6301 - val_acc: 0.8000\n",
      "Epoch 13/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.1828 - acc: 0.9350\n",
      "Epoch 00013: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.1828 - acc: 0.9350 - val_loss: 0.5993 - val_acc: 0.8000\n",
      "Epoch 14/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.1449 - acc: 0.9498\n",
      "Epoch 00014: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.1449 - acc: 0.9498 - val_loss: 0.8488 - val_acc: 0.7500\n",
      "Epoch 15/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.1079 - acc: 0.9626\n",
      "Epoch 00015: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.1079 - acc: 0.9626 - val_loss: 0.8340 - val_acc: 0.7500\n",
      "Epoch 16/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0748 - acc: 0.9750\n",
      "Epoch 00016: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0748 - acc: 0.9750 - val_loss: 0.9907 - val_acc: 0.7500\n",
      "Epoch 17/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0510 - acc: 0.9862\n",
      "Epoch 00017: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0510 - acc: 0.9862 - val_loss: 1.3973 - val_acc: 0.7500\n",
      "Epoch 18/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0377 - acc: 0.9898\n",
      "Epoch 00018: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.0377 - acc: 0.9898 - val_loss: 1.4142 - val_acc: 0.7500\n",
      "Epoch 19/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0262 - acc: 0.9920\n",
      "Epoch 00019: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.0262 - acc: 0.9920 - val_loss: 1.4542 - val_acc: 0.7500\n",
      "Epoch 20/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0280 - acc: 0.9920\n",
      "Epoch 00020: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0280 - acc: 0.9920 - val_loss: 1.6249 - val_acc: 0.7500\n",
      "Epoch 21/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0113 - acc: 0.9978\n",
      "Epoch 00021: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.0113 - acc: 0.9978 - val_loss: 1.7534 - val_acc: 0.7500\n",
      "Epoch 22/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0296 - acc: 0.9890\n",
      "Epoch 00022: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0296 - acc: 0.9890 - val_loss: 1.6403 - val_acc: 0.7500\n",
      "Epoch 23/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9988\n",
      "Epoch 00023: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0076 - acc: 0.9988 - val_loss: 1.8783 - val_acc: 0.7500\n",
      "Epoch 24/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0119 - acc: 0.9964\n",
      "Epoch 00024: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0119 - acc: 0.9964 - val_loss: 1.9723 - val_acc: 0.7250\n",
      "Epoch 25/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0254 - acc: 0.9930\n",
      "Epoch 00025: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0254 - acc: 0.9930 - val_loss: 1.7425 - val_acc: 0.7250\n",
      "Epoch 26/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0150 - acc: 0.9956\n",
      "Epoch 00026: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0150 - acc: 0.9956 - val_loss: 1.8172 - val_acc: 0.8000\n",
      "Epoch 27/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0094 - acc: 0.9974\n",
      "Epoch 00027: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 114s 114ms/step - loss: 0.0094 - acc: 0.9974 - val_loss: 1.8639 - val_acc: 0.7500\n",
      "Epoch 28/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0033 - acc: 0.9996\n",
      "Epoch 00028: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0033 - acc: 0.9996 - val_loss: 2.1320 - val_acc: 0.7500\n",
      "Epoch 29/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0132 - acc: 0.9962\n",
      "Epoch 00029: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.0132 - acc: 0.9962 - val_loss: 1.9181 - val_acc: 0.7250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0109 - acc: 0.9972\n",
      "Epoch 00030: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.0109 - acc: 0.9972 - val_loss: 1.9978 - val_acc: 0.7250\n",
      "Epoch 31/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0096 - acc: 0.9978\n",
      "Epoch 00031: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 117s 117ms/step - loss: 0.0096 - acc: 0.9978 - val_loss: 2.0046 - val_acc: 0.7000\n",
      "Epoch 32/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0118 - acc: 0.9964\n",
      "Epoch 00032: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.0118 - acc: 0.9964 - val_loss: 1.9657 - val_acc: 0.7500\n",
      "Epoch 33/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 9.6859e-04 - acc: 1.0000\n",
      "Epoch 00033: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 117s 117ms/step - loss: 9.6859e-04 - acc: 1.0000 - val_loss: 2.1885 - val_acc: 0.7250\n",
      "Epoch 34/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0193 - acc: 0.9934\n",
      "Epoch 00034: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0193 - acc: 0.9934 - val_loss: 2.2759 - val_acc: 0.7500\n",
      "Epoch 35/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0161 - acc: 0.9958\n",
      "Epoch 00035: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.0161 - acc: 0.9958 - val_loss: 1.9512 - val_acc: 0.7500\n",
      "Epoch 36/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0054 - acc: 0.9986\n",
      "Epoch 00036: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 117s 117ms/step - loss: 0.0054 - acc: 0.9986 - val_loss: 2.2399 - val_acc: 0.7500\n",
      "Epoch 37/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 5.6564e-04 - acc: 1.0000\n",
      "Epoch 00037: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 5.6564e-04 - acc: 1.0000 - val_loss: 2.6039 - val_acc: 0.7500\n",
      "Epoch 38/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0192 - acc: 0.9944\n",
      "Epoch 00038: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.0192 - acc: 0.9944 - val_loss: 1.9381 - val_acc: 0.7250\n",
      "Epoch 39/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 00039: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 2.3260 - val_acc: 0.7500\n",
      "Epoch 40/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0206 - acc: 0.9956\n",
      "Epoch 00040: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0206 - acc: 0.9956 - val_loss: 2.0553 - val_acc: 0.7500\n",
      "Epoch 41/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0040 - acc: 0.9990\n",
      "Epoch 00041: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0040 - acc: 0.9990 - val_loss: 2.3453 - val_acc: 0.7750\n",
      "Epoch 42/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 4.4670e-04 - acc: 1.0000\n",
      "Epoch 00042: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 4.4670e-04 - acc: 1.0000 - val_loss: 2.5112 - val_acc: 0.7750\n",
      "Epoch 43/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0176 - acc: 0.9950\n",
      "Epoch 00043: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.0176 - acc: 0.9950 - val_loss: 1.9468 - val_acc: 0.6500\n",
      "Epoch 44/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0037 - acc: 0.9994\n",
      "Epoch 00044: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.0037 - acc: 0.9994 - val_loss: 2.1701 - val_acc: 0.7000\n",
      "Epoch 45/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0046 - acc: 0.9984\n",
      "Epoch 00045: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0046 - acc: 0.9984 - val_loss: 2.2663 - val_acc: 0.7750\n",
      "Epoch 46/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0146 - acc: 0.9960\n",
      "Epoch 00046: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0146 - acc: 0.9960 - val_loss: 2.1354 - val_acc: 0.7500\n",
      "Epoch 47/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00047: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0028 - acc: 0.9994 - val_loss: 2.4012 - val_acc: 0.7500\n",
      "Epoch 48/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 3.9919e-04 - acc: 1.0000\n",
      "Epoch 00048: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 3.9919e-04 - acc: 1.0000 - val_loss: 2.6130 - val_acc: 0.7250\n",
      "Epoch 49/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0111 - acc: 0.9956\n",
      "Epoch 00049: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.0111 - acc: 0.9956 - val_loss: 2.2419 - val_acc: 0.7500\n",
      "Epoch 50/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 8.0681e-04 - acc: 1.0000\n",
      "Epoch 00050: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 8.0681e-04 - acc: 1.0000 - val_loss: 2.5332 - val_acc: 0.7250\n",
      "Epoch 51/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0111 - acc: 0.9956\n",
      "Epoch 00051: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 117s 117ms/step - loss: 0.0111 - acc: 0.9956 - val_loss: 2.4161 - val_acc: 0.7500\n",
      "Epoch 52/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9994\n",
      "Epoch 00052: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 117s 117ms/step - loss: 0.0025 - acc: 0.9994 - val_loss: 2.3441 - val_acc: 0.7000\n",
      "Epoch 53/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0038 - acc: 0.9992\n",
      "Epoch 00053: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 117s 117ms/step - loss: 0.0038 - acc: 0.9992 - val_loss: 2.6324 - val_acc: 0.7250\n",
      "Epoch 54/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.2904e-04 - acc: 1.0000\n",
      "Epoch 00054: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 117s 117ms/step - loss: 2.2904e-04 - acc: 1.0000 - val_loss: 2.9624 - val_acc: 0.7750\n",
      "Epoch 55/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.2843e-04 - acc: 1.0000\n",
      "Epoch 00055: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 1.2843e-04 - acc: 1.0000 - val_loss: inf - val_acc: 0.7500\n",
      "Epoch 56/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 7.2322e-05 - acc: 1.0000\n",
      "Epoch 00056: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 7.2322e-05 - acc: 1.0000 - val_loss: inf - val_acc: 0.7000\n",
      "Epoch 57/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0308 - acc: 0.9938\n",
      "Epoch 00057: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0308 - acc: 0.9938 - val_loss: 1.8034 - val_acc: 0.7500\n",
      "Epoch 58/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 00058: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 2.4022 - val_acc: 0.7250\n",
      "Epoch 59/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.2883e-04 - acc: 1.0000\n",
      "Epoch 00059: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 2.2883e-04 - acc: 1.0000 - val_loss: 2.7541 - val_acc: 0.7250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0064 - acc: 0.9980\n",
      "Epoch 00060: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0064 - acc: 0.9980 - val_loss: 2.5228 - val_acc: 0.7000\n",
      "Epoch 61/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0030 - acc: 0.9990\n",
      "Epoch 00061: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 2.4199 - val_acc: 0.7250\n",
      "Epoch 62/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.4774e-04 - acc: 1.0000\n",
      "Epoch 00062: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 1.4774e-04 - acc: 1.0000 - val_loss: 2.7929 - val_acc: 0.7250\n",
      "Epoch 63/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 9.2523e-05 - acc: 1.0000\n",
      "Epoch 00063: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 9.2523e-05 - acc: 1.0000 - val_loss: 2.9016 - val_acc: 0.7250\n",
      "Epoch 64/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 4.1388e-05 - acc: 1.0000\n",
      "Epoch 00064: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 117s 117ms/step - loss: 4.1388e-05 - acc: 1.0000 - val_loss: 3.0978 - val_acc: 0.7750\n",
      "Epoch 65/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0173 - acc: 0.9944\n",
      "Epoch 00065: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.0173 - acc: 0.9944 - val_loss: 2.2868 - val_acc: 0.7250\n",
      "Epoch 66/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 9.6671e-04 - acc: 1.0000\n",
      "Epoch 00066: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 9.6671e-04 - acc: 1.0000 - val_loss: 2.8227 - val_acc: 0.7250\n",
      "Epoch 67/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 3.2518e-04 - acc: 1.0000\n",
      "Epoch 00067: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 3.2518e-04 - acc: 1.0000 - val_loss: 3.0846 - val_acc: 0.7500\n",
      "Epoch 68/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.0025e-04 - acc: 1.0000\n",
      "Epoch 00068: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 1.0025e-04 - acc: 1.0000 - val_loss: inf - val_acc: 0.7500\n",
      "Epoch 69/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0173 - acc: 0.9948\n",
      "Epoch 00069: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.0173 - acc: 0.9948 - val_loss: 2.5961 - val_acc: 0.7500\n",
      "Epoch 70/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 4.3676e-04 - acc: 1.0000\n",
      "Epoch 00070: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 4.3676e-04 - acc: 1.0000 - val_loss: 2.7476 - val_acc: 0.7500\n",
      "Epoch 71/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0129 - acc: 0.9962\n",
      "Epoch 00071: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0129 - acc: 0.9962 - val_loss: 2.3114 - val_acc: 0.6750\n",
      "Epoch 72/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 4.3449e-04 - acc: 1.0000\n",
      "Epoch 00072: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 4.3449e-04 - acc: 1.0000 - val_loss: 2.6728 - val_acc: 0.7250\n",
      "Epoch 73/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.7622e-04 - acc: 1.0000\n",
      "Epoch 00073: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 1.7622e-04 - acc: 1.0000 - val_loss: 3.0290 - val_acc: 0.7750\n",
      "Epoch 74/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 9.2409e-05 - acc: 1.0000\n",
      "Epoch 00074: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 9.2409e-05 - acc: 1.0000 - val_loss: 3.0290 - val_acc: 0.7000\n",
      "Epoch 75/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 3.9218e-05 - acc: 1.0000\n",
      "Epoch 00075: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 3.9218e-05 - acc: 1.0000 - val_loss: 3.3142 - val_acc: 0.7500\n",
      "Epoch 76/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.2466e-05 - acc: 1.0000\n",
      "Epoch 00076: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 2.2466e-05 - acc: 1.0000 - val_loss: inf - val_acc: 0.7000\n",
      "Epoch 77/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.3470e-05 - acc: 1.0000\n",
      "Epoch 00077: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 117s 117ms/step - loss: 1.3470e-05 - acc: 1.0000 - val_loss: inf - val_acc: 0.7000\n",
      "Epoch 78/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0122 - acc: 0.9966\n",
      "Epoch 00078: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.0122 - acc: 0.9966 - val_loss: 2.5749 - val_acc: 0.7750\n",
      "Epoch 79/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.8636e-04 - acc: 1.0000\n",
      "Epoch 00079: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 1.8636e-04 - acc: 1.0000 - val_loss: 2.8219 - val_acc: 0.7750\n",
      "Epoch 80/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.1463e-04 - acc: 1.0000\n",
      "Epoch 00080: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 2.1463e-04 - acc: 1.0000 - val_loss: inf - val_acc: 0.7500\n",
      "Epoch 81/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9980\n",
      "Epoch 00081: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.0057 - acc: 0.9980 - val_loss: 2.9115 - val_acc: 0.7500\n",
      "Epoch 82/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.8159e-04 - acc: 1.0000\n",
      "Epoch 00082: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 1.8159e-04 - acc: 1.0000 - val_loss: 3.1336 - val_acc: 0.7250\n",
      "Epoch 83/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 8.3975e-05 - acc: 1.0000\n",
      "Epoch 00083: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 8.3975e-05 - acc: 1.0000 - val_loss: inf - val_acc: 0.7250\n",
      "Epoch 84/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 3.7642e-05 - acc: 1.0000\n",
      "Epoch 00084: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 3.7642e-05 - acc: 1.0000 - val_loss: inf - val_acc: 0.7250\n",
      "Epoch 85/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0283 - acc: 0.9948\n",
      "Epoch 00085: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0283 - acc: 0.9948 - val_loss: 2.6715 - val_acc: 0.7000\n",
      "Epoch 86/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 3.8562e-04 - acc: 1.0000\n",
      "Epoch 00086: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 3.8562e-04 - acc: 1.0000 - val_loss: 3.1242 - val_acc: 0.7250\n",
      "Epoch 87/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.4390e-04 - acc: 1.0000\n",
      "Epoch 00087: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 2.4390e-04 - acc: 1.0000 - val_loss: inf - val_acc: 0.7250\n",
      "Epoch 88/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 6.8052e-05 - acc: 1.0000\n",
      "Epoch 00088: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 6.8052e-05 - acc: 1.0000 - val_loss: inf - val_acc: 0.7250\n",
      "Epoch 89/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 6.0264e-05 - acc: 1.0000\n",
      "Epoch 00089: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 114s 114ms/step - loss: 6.0264e-05 - acc: 1.0000 - val_loss: inf - val_acc: 0.7250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.3825e-05 - acc: 1.0000\n",
      "Epoch 00090: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 2.3825e-05 - acc: 1.0000 - val_loss: inf - val_acc: 0.7250\n",
      "Epoch 91/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.0151e-05 - acc: 1.0000\n",
      "Epoch 00091: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 1.0151e-05 - acc: 1.0000 - val_loss: inf - val_acc: 0.7250\n",
      "Epoch 92/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0193 - acc: 0.9948\n",
      "Epoch 00092: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.0193 - acc: 0.9948 - val_loss: 2.2909 - val_acc: 0.7250\n",
      "Epoch 93/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.6166e-04 - acc: 1.0000\n",
      "Epoch 00093: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 2.6166e-04 - acc: 1.0000 - val_loss: 2.6087 - val_acc: 0.7000\n",
      "Epoch 94/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.5176e-04 - acc: 1.0000\n",
      "Epoch 00094: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 1.5176e-04 - acc: 1.0000 - val_loss: 2.7747 - val_acc: 0.7250\n",
      "Epoch 95/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 6.1983e-05 - acc: 1.0000\n",
      "Epoch 00095: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 6.1983e-05 - acc: 1.0000 - val_loss: inf - val_acc: 0.7250\n",
      "Epoch 96/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 6.1104e-04 - acc: 0.9998\n",
      "Epoch 00096: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 6.1104e-04 - acc: 0.9998 - val_loss: inf - val_acc: 0.7750\n",
      "Epoch 97/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0136 - acc: 0.9960\n",
      "Epoch 00097: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: 0.0136 - acc: 0.9960 - val_loss: inf - val_acc: 0.7500\n",
      "Epoch 98/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.4599e-04 - acc: 1.0000\n",
      "Epoch 00098: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 1.4599e-04 - acc: 1.0000 - val_loss: inf - val_acc: 0.7250\n",
      "Epoch 99/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.0287e-04 - acc: 1.0000\n",
      "Epoch 00099: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 1.0287e-04 - acc: 1.0000 - val_loss: inf - val_acc: 0.7250\n",
      "Epoch 100/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 4.6745e-05 - acc: 1.0000\n",
      "Epoch 00100: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 117s 117ms/step - loss: 4.6745e-05 - acc: 1.0000 - val_loss: inf - val_acc: 0.7500\n",
      "Epoch 101/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 2.2706e-05 - acc: 1.0000\n",
      "Epoch 00101: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 2.2706e-05 - acc: 1.0000 - val_loss: inf - val_acc: 0.7500\n",
      "Epoch 102/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: nan - acc: 0.9862\n",
      "Epoch 00102: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: nan - acc: 0.9862 - val_loss: nan - val_acc: 0.8250\n",
      "Epoch 103/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: nan - acc: 0.8094\n",
      "Epoch 00103: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 114s 114ms/step - loss: nan - acc: 0.8094 - val_loss: nan - val_acc: 0.8250\n",
      "Epoch 104/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: nan - acc: 0.8090\n",
      "Epoch 00104: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 117s 117ms/step - loss: nan - acc: 0.8090 - val_loss: nan - val_acc: 0.8250\n",
      "Epoch 105/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: nan - acc: 0.8098\n",
      "Epoch 00105: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 115s 115ms/step - loss: nan - acc: 0.8098 - val_loss: nan - val_acc: 0.8250\n",
      "Epoch 106/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: nan - acc: 0.8094\n",
      "Epoch 00106: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: nan - acc: 0.8094 - val_loss: nan - val_acc: 0.8250\n",
      "Epoch 107/250\n",
      "1000/1000 [==============================] - ETA: 0s - loss: nan - acc: 0.8084\n",
      "Epoch 00107: val_loss did not improve from 0.43137\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: nan - acc: 0.8084 - val_loss: nan - val_acc: 0.8250\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/CIB_0_.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7059d0aa4f9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m         callbacks=[checkpoint_cb, early_stopping_cb])\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'CIB_{i}_.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/CIB_0_.pkl'"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    # call model\n",
    "    input_dim = np.expand_dims(X_train, axis = -1).shape[1:]\n",
    "    #input_dim = np.expand_dims(X_train, axis = -1).shape[1:5]\n",
    "    output_dim = C-1\n",
    "    model_3d = ontram(img_model_linear_final(input_dim, output_dim))         \n",
    "\n",
    "    # Define data loaders.\n",
    "    train_loader = tf.data.Dataset.from_tensor_slices((X_train[:ntrain], Y_train_MRS[:ntrain]))\n",
    "    validation_loader = tf.data.Dataset.from_tensor_slices((X_valid[:nvalid], Y_valid_MRS[:nvalid]))\n",
    "\n",
    "    # data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.15,\n",
    "        shear_range=0.15,\n",
    "        fill_mode=\"nearest\")\n",
    "    datagen.fit(X_train[:ntrain])\n",
    "\n",
    "    validation_dataset = (\n",
    "        validation_loader.shuffle(len(X_valid[:nvalid]))\n",
    "        .map(validation_preprocessing)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(2)\n",
    "    )\n",
    "\n",
    "    #compile\n",
    "    model_3d.compile(optimizer=keras.optimizers.Adam(learning_rate=5*1e-5),\n",
    "                            loss=ontram_loss(C, batch_size),\n",
    "                            metrics=[ontram_acc(C, batch_size)])\n",
    "    \n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    filepath = os.path.join('/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/', f'CIB_{i}_.h5'),\n",
    "    verbose = 1,\n",
    "    save_weights_only = True,\n",
    "    monitor = \"val_loss\", #'val_acc',\n",
    "    mode = 'min',\n",
    "    save_best_only = True)\n",
    "\n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=100, restore_best_weights=True)\n",
    "    \n",
    "    hist = model_3d.fit(\n",
    "        datagen.flow(X_train[:ntrain], Y_train_MRS[:ntrain], batch_size=batch_size, shuffle=True),\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=250,\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        steps_per_epoch = 1000,        \n",
    "        callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "    \n",
    "    #pkl.dump(hist.history, open(os.path.join('/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/', f'CIB_{i}_.pkl')), protocol=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CI_B & LS_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for preprocessing\n",
    "def train_preprocessing2(data, label):\n",
    "    \"\"\"Process training data.\"\"\"\n",
    "    volume = data[0] # intercept: image\n",
    "    tabular = data[1] # shift: tabular\n",
    "    volume = zoom(volume)\n",
    "    volume = rotate(volume)\n",
    "    volume = shift(volume)\n",
    "    volume = flip(volume)\n",
    "    return (volume, tabular), label\n",
    "\n",
    "# Define data sets\n",
    "train_data_2 = tf.data.Dataset.from_tensor_slices((X_train[:ntrain], X_tab_train[:ntrain]))\n",
    "train_labels_2 = tf.data.Dataset.from_tensor_slices((Y_train_MRS[:ntrain]))\n",
    "\n",
    "valid_data_2 = tf.data.Dataset.from_tensor_slices((X_valid[:ntrain], X_tab_valid[:ntrain]))\n",
    "valid_labels_2 = tf.data.Dataset.from_tensor_slices((Y_valid_MRS[:nvalid]))\n",
    "\n",
    "test_data_2 = tf.data.Dataset.from_tensor_slices((X_test[:ntest], X_tab_test[:ntest]))\n",
    "test_labels_2 = tf.data.Dataset.from_tensor_slices((Y_test_MRS[:ntest]))\n",
    "\n",
    "train_loader_2 = tf.data.Dataset.zip((train_data_2, train_labels_2))\n",
    "validation_loader_2 = tf.data.Dataset.zip((valid_data_2, valid_labels_2))\n",
    "test_loader_2 = tf.data.Dataset.zip((test_data_2, test_labels_2))\n",
    "\n",
    "train_dataset_MRS_2 = (train_loader_2.shuffle(ntrain)\n",
    "                 .map(train_preprocessing2)\n",
    "                 .batch(batch_size))\n",
    "\n",
    "validation_dataset_MRS_2 = (validation_loader_2.batch(batch_size))\n",
    "\n",
    "test_dataset_MRS_2 = (test_loader_2.batch(len(X_test[:ntest])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_CIBLSX = True\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fit_CIBLSX:\n",
    "    for i in range(5):\n",
    "        CI_B = img_model_linear(X_train.shape[1:], 1)\n",
    "        LS_X = mod_linear_shift(X_tab_train.shape[1])\n",
    "        mod_ontram_CIBLSX = ontram(CI_B, LS_X)\n",
    "        mod_ontram_CIBLSX.compile(optimizer = keras.optimizers.Adam(),\n",
    "                    loss = ontram_loss(C, batch_size),\n",
    "                    metrics = [ontram_acc(C, batch_size)])    \n",
    "\n",
    "        checkpoint2 = ModelCheckpoint(os.path.join('/tf/notebooks/schnemau/xAI_stroke_3d/ensembling_results/', f'model_{i}_{i}_ontram_CIBLSX_.h5'),\n",
    "                                        save_best_only=True, save_weights_only=True)\n",
    "        \n",
    "        early_stopping2 = EarlyStopping(patience=100, restore_best_weights=True)\n",
    "        \n",
    "        \n",
    "        mod_ontram_CIBLSX.fit(train_dataset_MRS_2, \n",
    "                validation_data = validation_dataset_MRS_2,\n",
    "                epochs = 250,\n",
    "                shuffle = True,\n",
    "                callbacks=[checkpoint2, early_stopping2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jonas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_jonas = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_jonas:\n",
    "        for i in range(5):\n",
    "                jon_softmax = stroke_binary_3d_softmax()\n",
    "                jon_softmax.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                optimizer=keras.optimizers.Adam(learning_rate=5*1e-5),\n",
    "                metrics=[\"acc\", tf.keras.metrics.AUC()])   \n",
    "\n",
    "                checkpoint3 = ModelCheckpoint(os.path.join('/tf/notebooks/schnemau/xAI_stroke_3d/ensembling_results/', f'model_jonas_{i}.h5'),\n",
    "                                        save_best_only=True, save_weights_only=True)\n",
    "        \n",
    "                early_stopping3 = EarlyStopping(patience=7, restore_best_weights=True)  \n",
    "\n",
    "                jon_softmax.fit(train_dataset_MRS, \n",
    "                    validation_data = validation_dataset_MRS,\n",
    "                    epochs = 35,\n",
    "                    shuffle = True,\n",
    "                    callbacks=[checkpoint3, early_stopping3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonas from earlier check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/tf/notebooks/schnemau/xAI_stroke_3d/weights/andrea_split/3d_cnn_binary_model_split6_unnormalized_avg_layer_paper_model_sigmoid_activation_14.h5'\n",
    "\n",
    "input_dim = X_train.shape[1:]\n",
    "output_dim = 1\n",
    "\n",
    "layer_connection = \"globalAveragePooling\"\n",
    "last_activation = \"sigmoid\"\n",
    "\n",
    "# call model\n",
    "model_3d2 = md.stroke_binary_3d(input_dim = input_dim,\n",
    "                               output_dim = output_dim,\n",
    "                               layer_connection = layer_connection,\n",
    "                               last_activation = last_activation)\n",
    "model_3d2.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=5*1e-5),\n",
    "    metrics=[\"acc\", tf.keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "model_3d2.load_weights(path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
