{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold #?\n",
    "from sklearn import preprocessing #?\n",
    "from sklearn import metrics #?\n",
    "from scipy import ndimage #?\n",
    "\n",
    "\n",
    "#import functions_read_data as rdat\n",
    "# Tensorflow/Keras\n",
    "import tensorflow as tf\n",
    "import time\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pickle as pkl\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"/tf/notebooks/schnemau/xAI_stroke_3d/\"\n",
    "os.chdir(DIR)\n",
    "DATA_DIR = DIR + \"data/\"\n",
    "OUTPUT_DIR = DIR + \"weights/10Fold_CIB/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ontram functions\n",
    "from k_ontram_functions.ontram import ontram\n",
    "from k_ontram_functions.ontram_loss import ontram_loss\n",
    "from k_ontram_functions.ontram_metrics import ontram_acc, ontram_auc\n",
    "from k_ontram_functions.ontram_predict import predict_ontram, get_parameters\n",
    "#data augmentation techniques\n",
    "from functions.augmentation3d import zoom, rotate, flip, shift\n",
    "from functions.plot_slices import plot_slices\n",
    "import functions_read_data as rdat\n",
    "#my own functions\n",
    "import Utils_maurice as utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(407, 14)\n",
      "(407, 128, 128, 28)\n"
     ]
    }
   ],
   "source": [
    "id_tab = pd.read_csv(DATA_DIR + \"10Fold_ids_V0.csv\", sep=\",\") \n",
    "X = np.load(DATA_DIR + \"prepocessed_dicom_3d.npy\")\n",
    "model_version = 1 # define the model version\n",
    "print(id_tab.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True \n",
    "\n",
    "# Define Model\n",
    "layer_connection = \"globalAveragePooling\" \n",
    "last_activation = \"linear\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 10   # 10 for all models\n",
    "num_models = 5    # see overview of all models (above)\n",
    "\n",
    "batch_size = 6    # 6 for all models\n",
    "epochs = 250      # 250 for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocessing(data, label):\n",
    "    volume = data  \n",
    "    volume = zoom(volume)\n",
    "    volume = rotate(volume)\n",
    "    volume = shift(volume)\n",
    "    volume = flip(volume)\n",
    "    return (volume), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get real about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "################################################################################\n",
      "Split 0\n",
      "################################################################################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#######################################################\n",
      "Split 0 Model 0\n",
      "#######################################################\n",
      "\n",
      "\n",
      "Epoch 1/250\n",
      " 6/54 [==>...........................] - ETA: 25s - loss: 0.7013 - acc: 0.4444"
     ]
    }
   ],
   "source": [
    "# loop over splits (kFold)\n",
    "\n",
    "start0 = time.time()\n",
    "for i in range(num_splits):\n",
    "    start1 = time.time()\n",
    "    print(\"\\n\\n\\n\\n################################################################################\")\n",
    "    print(\"Split \" + str(i))\n",
    "    print(\"################################################################################\\n\\n\\n\\n\")\n",
    "    \n",
    "    (X_train, X_valid, X_test), (Y_train, Y_valid, Y_test) = rdat.split_data(id_tab, X, i)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=-1)\n",
    "    X_valid = np.expand_dims(X_valid, axis=-1)\n",
    "    X_test = np.expand_dims(X_test, axis=-1)\n",
    "    Y_train = to_categorical(Y_train)\n",
    "    Y_valid = to_categorical(Y_valid)\n",
    "    Y_test = to_categorical(Y_test)\n",
    "    \n",
    "    input_dim = X_train.shape[1:]\n",
    "    C = Y_train.shape[1]\n",
    "    output_dim = Y_train.shape[1]-1\n",
    "    (input_dim, output_dim, C)\n",
    "\n",
    "\n",
    "    # loop over model instances (ensembling)\n",
    "    \n",
    "    for j in range(num_models):\n",
    "        start2 = time.time()\n",
    "        print(\"\\n\\n#######################################################\")\n",
    "        print(\"Split \" + str(i) + \" Model \" + str(j))\n",
    "        print(\"#######################################################\\n\\n\")      \n",
    "    \n",
    "        model_3d = ontram(utils.img_model_linear_final(input_dim, output_dim))     \n",
    "        \n",
    "        model_3d.compile(optimizer=keras.optimizers.Adam(learning_rate=5*1e-5),\n",
    "                                        loss=ontram_loss(C, batch_size),\n",
    "                                        metrics=[ontram_acc(C, batch_size)])\n",
    "\n",
    "        model_name = (\"3d_cnn_binary_model_split\" + str(i) + \n",
    "                          \"_unnormalized_avg_layer_paper_model_\" + last_activation + \"_activation_\" + str(model_version) + \"_\" + str(j) + \".h5\")\n",
    "\n",
    "\n",
    "        # Train\n",
    "        train_data = tf.data.Dataset.from_tensor_slices((X_train))\n",
    "        train_labels = tf.data.Dataset.from_tensor_slices((Y_train))\n",
    "        # Valid\n",
    "        valid_data = tf.data.Dataset.from_tensor_slices((X_valid))\n",
    "        valid_labels = tf.data.Dataset.from_tensor_slices((Y_valid))\n",
    "        # Zip\n",
    "        train_loader = tf.data.Dataset.zip((train_data, train_labels))\n",
    "        validation_loader = tf.data.Dataset.zip((valid_data, valid_labels))           \n",
    "        # Data-Sets\n",
    "        train_dataset = (train_loader.shuffle(len(X_train))\n",
    "                    .map(train_preprocessing)\n",
    "                    .batch(batch_size, drop_remainder=True))\n",
    "\n",
    "        validation_dataset = (validation_loader.batch(batch_size, drop_remainder = True))     \n",
    "               \n",
    "         \n",
    "        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "            filepath = OUTPUT_DIR + model_name,\n",
    "            verbose = (1 if i == 0 and j == 0 else 0),\n",
    "            save_weights_only = True,\n",
    "            monitor = \"val_loss\", #'val_acc',\n",
    "            mode = 'min',\n",
    "            save_best_only = True)\n",
    "\n",
    "        early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=150, restore_best_weights=True)\n",
    "        \n",
    "        \n",
    "        # Train the model, doing validation at the end of each epoch\n",
    "        if train:\n",
    "            hist = model_3d.fit(\n",
    "            train_dataset,\n",
    "            validation_data=validation_dataset,\n",
    "            epochs=250,\n",
    "            shuffle=True,\n",
    "            verbose=(1 if i == 0 and j == 0 else 0), \n",
    "            callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "            pkl.dump(hist.history, open(OUTPUT_DIR + \"hist_\" + model_name[:-2] + \"pkl\", \"wb\"), protocol=4)            \n",
    "        \n",
    "        end2 = time.time()\n",
    "        print(\" \")   \n",
    "        print(\"Duration of Training: \" + str(end2-start2))  \n",
    "        \n",
    "    end1 = time.time()\n",
    "    print(\" \")   \n",
    "    print(\"Duration of Split: \" + str(end1-start1))  \n",
    "        \n",
    "end0 = time.time()\n",
    "print(\" \")\n",
    "print(\"Duration of Everything: \" + str(end0-start0))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
