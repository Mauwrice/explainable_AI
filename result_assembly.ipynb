{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results of all Models for a given Data and Model Version\n",
    "\n",
    "Loop over all Folds and Models and save the results in a dataframe.  \n",
    "Calculate a threshold for outcome prediction on the validation data (based on geometric mean) and apply it to the test data.  \n",
    "Add a normalized prediction uncertainty based on the standard deviation of the predictions of the different models.  \n",
    "Check the calibrartion plots.\n",
    "\n",
    "## Load Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, log_loss\n",
    "from sklearn import metrics\n",
    "from skimage import exposure\n",
    "from scipy import ndimage\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "print(\"TF  Version\",tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and set path before loading modules\n",
    "print(os.getcwd())\n",
    "DIR = \"/tf/notebooks/brdd/xAI_stroke_3d/\"\n",
    "if os.getcwd() != DIR:\n",
    "    os.chdir(DIR)\n",
    "    print(os.getcwd())\n",
    "\n",
    "import functions_metrics as fm\n",
    "import functions_model_definition as md\n",
    "import functions_read_data as rdat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data Version (which splits and models)\n",
    "version = \"10Fold_sigmoid_V0\" # one of the following (see ZZ_overview_of_runs.txt for details):\n",
    "# 10Fold_sigmoid_V0, 10Fold_sigmoid_V1, 10Fold_sigmoid_V2, 10Fold_sigmoid_V2f, 10Fold_sigmoid_V3  \n",
    "# 10Fold_softmax_V0, 10Fold_softmax_V1, andrea\n",
    "\n",
    "# Define Model Version\n",
    "model_version = 2\n",
    "\n",
    "# should csv be saved?\n",
    "save_file = False\n",
    "\n",
    "DATA_OUTPUT_DIR = DIR + \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the paths for the result assembly\n",
    "\n",
    "## 10 Fold\n",
    "if version.startswith(\"10Fold\"):\n",
    "    # Define the path + output path:\n",
    "\n",
    "    DATA_DIR = DIR + \"data/\"\n",
    "    WEIGHT_DIR = DIR + \"weights/\" + version + \"/\"\n",
    "\n",
    "    # load data\n",
    "    if version.endswith(\"V0\") or version.endswith(\"sigmoid\"):\n",
    "        id_tab = pd.read_csv(DATA_DIR + \"10Fold_ids_V0.csv\", sep=\",\")\n",
    "    elif version.endswith(\"V1\"):\n",
    "        id_tab = pd.read_csv(DATA_DIR + \"10Fold_ids_V1.csv\", sep=\",\")\n",
    "    elif version.endswith(\"V2\") or version.endswith(\"V2f\"):\n",
    "        id_tab = pd.read_csv(DATA_DIR + \"10Fold_ids_V2.csv\", sep=\",\")\n",
    "    elif version.endswith(\"V3\"):\n",
    "        id_tab = pd.read_csv(DATA_DIR + \"10Fold_ids_V3.csv\", sep=\",\")\n",
    "    X = np.load(DATA_DIR + \"prepocessed_dicom_3d.npy\")\n",
    "\n",
    "    # define save name\n",
    "    all_result_name = \"all_tab_results_\" + version + \"_M\" + str(model_version)\n",
    "\n",
    "    # define splits\n",
    "    which_splits = list(range(0,10)) # 10 Fold\n",
    "\n",
    "    print(id_tab.shape)\n",
    "    print(X.shape)\n",
    "\n",
    "    \n",
    "## Andrea 6 Split\n",
    "elif version == \"andrea\":\n",
    "    # Define the path + output path:\n",
    "\n",
    "    IMG_DIR = \"/tf/notebooks/hezo/stroke_zurich/data/\" \n",
    "    DATA_DIR = \"/tf/notebooks/hezo/stroke_zurich/data/\" \n",
    "    WEIGHT_DIR = DIR + \"weights/andrea_split/\"\n",
    "\n",
    "    # define data paths\n",
    "    img_path = IMG_DIR + 'dicom_2d_192x192x3_clean_interpolated_18_02_2021_preprocessed2.h5'\n",
    "    tab_path = IMG_DIR + 'baseline_data_zurich_prepared.csv'\n",
    "    split_path = DIR + 'data/andrea_splits.csv'\n",
    "\n",
    "    # define save name\n",
    "    all_result_name = \"all_tab_results_andrea_split\"\n",
    "\n",
    "    # # define splits\n",
    "    which_splits = list(range(1,7)) # Andrea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(WEIGHT_DIR)\n",
    "print(all_result_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## define model numbers\n",
    "# if version in [\"andrea\", \"10Fold_sigmoid_V0\", \"10Fold_softmax_V0\", \n",
    "#                \"10Fold_sigmoid_V2\", \"10Fold_sigmoid_V2f\", \"10Fold_sigmoid_V3\"]:\n",
    "#     model_nrs = list(range(model_version*10, model_version*10+5))\n",
    "# elif version in [\"10Fold_softmax_V1\", \"10Fold_sigmoid_V1\"]:\n",
    "#     model_nrs = list(range(model_version*10, model_version*10+10))\n",
    "\n",
    "# # define model\n",
    "# input_dim = (128, 128, 28, 1)\n",
    "\n",
    "# if \"sigmoid\" in version or \"andrea_split\" in version:\n",
    "#     last_activation = \"sigmoid\"\n",
    "#     output_dim = 1\n",
    "#     LOSS = \"binary_crossentropy\"\n",
    "# elif \"softmax\" in version:\n",
    "#     last_activation = \"softmax\"\n",
    "#     output_dim = 2\n",
    "#     LOSS = tf.keras.losses.categorical_crossentropy\n",
    "    \n",
    "# if version.endswith(\"f\"):\n",
    "#     layer_connection = \"flatten\"\n",
    "# else:\n",
    "#     layer_connection = \"globalAveragePooling\"\n",
    "    \n",
    "(input_dim, output_dim, LOSS, layer_connection, last_activation) = md.model_setup(version)\n",
    "    \n",
    "\n",
    "model_3d = md.stroke_binary_3d(input_dim = input_dim,\n",
    "                               output_dim = output_dim,\n",
    "                               layer_connection = layer_connection,\n",
    "                               last_activation = last_activation)\n",
    "model_3d.compile(\n",
    "    loss=LOSS,\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=5*1e-5),\n",
    "    metrics=[\"acc\", tf.keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "# define if print should be enabled\n",
    "check_print = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## define model numbers\n",
    "if version in [\"andrea\", \"10Fold_sigmoid_V0\", \"10Fold_softmax_V0\", \n",
    "               \"10Fold_sigmoid_V2\", \"10Fold_sigmoid_V2f\", \"10Fold_sigmoid_V3\"]:\n",
    "    model_nrs = list(range(5))\n",
    "elif version in [\"10Fold_softmax_V1\", \"10Fold_sigmoid_V1\"]:\n",
    "    model_nrs = list(range(10))\n",
    "\n",
    "# Define model name\n",
    "generate_model_name = md.set_generate_model_name(\n",
    "    model_version = model_version, \n",
    "    layer_connection = layer_connection, \n",
    "    last_activation = last_activation, \n",
    "    path = WEIGHT_DIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = [] # test dataset results\n",
    "valid_list = [] # validation datset results\n",
    "\n",
    "# loop over splits and models\n",
    "start = time.time()\n",
    "for which_split in which_splits:\n",
    "    if check_print:\n",
    "        print(\" \")\n",
    "        print(\"---- Start Reading Data of Split \" + str(which_split) + \" ----\")\n",
    "        print(\" \")\n",
    "    \n",
    "    # 10 Fold\n",
    "    if version.startswith(\"10Fold\"):\n",
    "        (X_train, X_valid, X_test), (y_train, y_valid, y_test) = rdat.split_data(id_tab, X, which_split)\n",
    "        results = id_tab[id_tab[\"fold\" + str(which_split)] == \"test\"].copy()\n",
    "    \n",
    "    # Andrea\n",
    "    elif version == \"andrea\":\n",
    "        (X_train, X_valid, X_test), (y_train, y_valid, y_test), results = rdat.read_and_split_img_data_andrea(\n",
    "                path_img = img_path, \n",
    "                path_tab = tab_path, \n",
    "                path_splits = split_path, \n",
    "                split = which_split, \n",
    "                check_print = check_print)\n",
    "           \n",
    "    # add variable with current split\n",
    "    results[\"test_split\"] = which_split        \n",
    "    \n",
    "    # create new df for validation set, to calculate classification threshold \n",
    "    # => not cheating when calc on valid\n",
    "    validation_results = pd.DataFrame(\n",
    "        {\"test_split\": which_split,\n",
    "         \"unfavorable\": y_valid}\n",
    "    )\n",
    "    \n",
    "    if check_print:\n",
    "        print(\" \")\n",
    "        print(\"---- Starting Result Calculation of Split \" + str(which_split) + \" ----\")\n",
    "        print(\" \")\n",
    "        \n",
    "    y_test_preds = []\n",
    "    y_valid_preds = []\n",
    "     \n",
    "    for model_nr in model_nrs:\n",
    "        if check_print:\n",
    "            print(\"Now calculating model nr. \" + str(model_nr))\n",
    "        \n",
    "        model_3d.load_weights(generate_model_name(which_split, model_nr))\n",
    "        \n",
    "        if last_activation == \"softmax\":\n",
    "            y_test_preds.append(model_3d.predict(X_test)[:,1].squeeze())\n",
    "        else:\n",
    "            y_test_preds.append(model_3d.predict(X_test).squeeze())\n",
    "        results[\"y_pred_model_\" + str(model_nr)] = y_test_preds[-1]\n",
    "        \n",
    "        if last_activation == \"softmax\":\n",
    "            y_valid_preds.append(model_3d.predict(X_valid)[:,1].squeeze())\n",
    "        else:\n",
    "            y_valid_preds.append(model_3d.predict(X_valid).squeeze())\n",
    "        validation_results[\"y_pred_model_\" + str(model_nr)] = y_valid_preds[-1]\n",
    "    \n",
    "    y_test_preds = np.array(y_test_preds)\n",
    "    results[\"y_pred_linear_avg\"] = np.mean(y_test_preds, axis = 0)\n",
    "    results[\"y_pred_trafo_avg\"] = fm.sigmoid(np.mean(fm.inverse_sigmoid(y_test_preds), axis = 0))\n",
    "    \n",
    "    y_valid_preds = np.array(y_valid_preds)\n",
    "    validation_results[\"y_pred_linear_avg\"] = np.mean(y_valid_preds, axis = 0)\n",
    "    validation_results[\"y_pred_trafo_avg\"] = fm.sigmoid(np.mean(fm.inverse_sigmoid(y_valid_preds), axis = 0))\n",
    "        \n",
    "    results_list.append(results)\n",
    "    valid_list.append(validation_results)\n",
    "        \n",
    "if check_print:\n",
    "    end = time.time()\n",
    "    print(\" \")\n",
    "    print(\"---- DONE ----\")\n",
    "    print(\" \")   \n",
    "    print(\"Duration of Execution: \" + str(end-start))               \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Threshold\n",
    "\n",
    "Calculation of threshold for classification is done on validation data. Then applied to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should threshold be calculated per split or over all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_thresholds = []\n",
    "\n",
    "for i, validation_results in enumerate(valid_list):\n",
    "    print(\" \")\n",
    "    print(\"---- Split \" + str(which_splits[i]) + \" ----\")\n",
    "    print(\" \")\n",
    "    \n",
    "    y_org = validation_results[\"unfavorable\"]\n",
    "    y_pred = validation_results[\"y_pred_trafo_avg\"]\n",
    "    \n",
    "    # calculate fpr, tpr and thresholds\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_org, y_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    # calculate geometric mean of tpr and fpr to find best threshold\n",
    "    gmean = np.sqrt(tpr * (1 - fpr))\n",
    "\n",
    "    # Find the optimal threshold\n",
    "    index = np.argmax(gmean)\n",
    "    valid_thresholds.append(threshold[index])\n",
    "    print(\"Optimal Geometric Mean Threshold: \" + str(threshold[index]))\n",
    "    \n",
    "    # Calc Acc\n",
    "    y_pred_label = (y_pred >= threshold[index]).squeeze()\n",
    "    print(\"Accuracy to beat: \" + str(1 - np.mean(y_org)))\n",
    "    print(\"Accuracy: \" + str(np.mean(y_pred_label == y_org)))\n",
    "          \n",
    "    print(\"Spezifität: \", 1-fpr[index])\n",
    "    print(\"Sensitivität: \", tpr[index])\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10,5))\n",
    "    \n",
    "    # method I: plt\n",
    "    ax1.title.set_text('Receiver Operating Characteristic')\n",
    "    ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    ax1.scatter(fpr[index], tpr[index], color = \"red\", s = 50)\n",
    "    ax1.legend(loc = 'lower right')\n",
    "    ax1.plot([0, 1], [0, 1],'r--')\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    \n",
    "    sns.boxplot(x = \"unfavorable\",\n",
    "        y = \"y_pred_trafo_avg\",\n",
    "        data = validation_results,\n",
    "        ax = ax2)\n",
    "    sns.stripplot(x = \"unfavorable\",\n",
    "        y = \"y_pred_trafo_avg\",\n",
    "        color = 'blue',\n",
    "        data = validation_results,\n",
    "        ax = ax2)\n",
    "    ax2.axhline(y = threshold[index], color = \"red\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_thresholds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Threshold to Testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, results in enumerate(results_list):\n",
    "    print(\" \")\n",
    "    print(\"---- Split \" + str(which_splits[i]) + \" ----\")\n",
    "    print(\" \")\n",
    "\n",
    "    results[\"threshold\"] = valid_thresholds[i]\n",
    "    results[\"y_pred_class\"] = (results[\"y_pred_trafo_avg\"] >= results[\"threshold\"]).astype(int)\n",
    "    \n",
    "    y_org = results[\"unfavorable\"]\n",
    "    y_pred = results[\"y_pred_class\"]\n",
    "    \n",
    "    # calculate fpr, tpr and thresholds\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_org, y_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "      \n",
    "    # Calc Acc\n",
    "    print(\"Accuracy to beat: \" + str(1 - np.mean(y_org)))\n",
    "    print(\"Accuracy: \" + str(np.mean(y_pred == y_org)))\n",
    "          \n",
    "    print(\"Spezifität: \", 1-fpr[1])\n",
    "    print(\"Sensitivität: \", tpr[1])\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10,5))\n",
    "    \n",
    "    # method I: plt\n",
    "    ax1.title.set_text('Receiver Operating Characteristic')\n",
    "    ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    ax1.legend(loc = 'lower right')\n",
    "    ax1.plot([0, 1], [0, 1],'r--')\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    \n",
    "    sns.boxplot(x = \"unfavorable\",\n",
    "        y = \"y_pred_trafo_avg\",\n",
    "        data = results,\n",
    "        ax = ax2)\n",
    "    sns.stripplot(x = \"unfavorable\",\n",
    "        y = \"y_pred_trafo_avg\",\n",
    "        color = 'blue',\n",
    "        data = results,\n",
    "        ax = ax2)\n",
    "    ax2.axhline(y = valid_thresholds[i], color = \"red\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat all Pandas and Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat(results_list)\n",
    "all_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_org = all_results[\"unfavorable\"]\n",
    "y_pred = all_results[\"y_pred_class\"]\n",
    "\n",
    "# calculate fpr, tpr and thresholds\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_org, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# Calc Acc\n",
    "y_pred_label = all_results[\"y_pred_class\"]\n",
    "print(\"Accuracy to beat: \" + str(1 - np.mean(y_org)))\n",
    "print(\"Accuracy: \" + str(np.mean(y_pred_label == y_org)))\n",
    "\n",
    "print(\"Spezifität: \", 1-fpr[1])\n",
    "print(\"Sensitivität: \", tpr[1])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10,5))\n",
    "\n",
    "# method I: plt\n",
    "ax1.title.set_text('Receiver Operating Characteristic')\n",
    "ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "ax1.legend(loc = 'lower right')\n",
    "ax1.plot([0, 1], [0, 1],'r--')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "\n",
    "sns.boxplot(x = \"unfavorable\",\n",
    "    y = \"y_pred_trafo_avg\",\n",
    "    data = all_results,\n",
    "    ax = ax2)\n",
    "sns.stripplot(x = \"unfavorable\",\n",
    "    y = \"y_pred_trafo_avg\",\n",
    "    hue = 'y_pred_class',\n",
    "    palette=[\"C2\", \"C3\", \"k\"],\n",
    "    data = all_results,\n",
    "    alpha = 0.75,\n",
    "    ax = ax2)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of one threshold\n",
    "\n",
    "1. Threshold based on all validation data. This makes not much sense as each fold has different models.\n",
    "2. Threshold based on the validation data of each fold. This approach is used in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "all_valid_results = pd.concat(valid_list)\n",
    "\n",
    "y_org = all_valid_results[\"unfavorable\"]\n",
    "y_pred = all_valid_results[\"y_pred_trafo_avg\"]\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_org, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# calculate geometric mean of tpr and fpr to find best threshold\n",
    "gmean = np.sqrt(tpr * (1 - fpr))\n",
    "\n",
    "# Find the optimal threshold\n",
    "index = np.argmax(gmean)\n",
    "valid_threshold = threshold[index]\n",
    "print(\"threshold: \", threshold[index])\n",
    "\n",
    "\n",
    "y_pred_label = (y_pred >= threshold[index]).squeeze()\n",
    "print(\"Accuracy to beat: \" + str(1 - np.mean(y_org)))\n",
    "print(\"Accuracy: \" + str(np.mean(y_pred_label == y_org)))\n",
    "\n",
    "print(\"Spezifität: \", 1-fpr[index])\n",
    "print(\"Sensitivität: \", tpr[index])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10,5))\n",
    "\n",
    "# method I: plt\n",
    "ax1.title.set_text('Receiver Operating Characteristic')\n",
    "ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "ax1.scatter(fpr[index], tpr[index], color = \"red\", s = 50)\n",
    "ax1.legend(loc = 'lower right')\n",
    "ax1.plot([0, 1], [0, 1],'r--')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "\n",
    "sns.boxplot(x = \"unfavorable\",\n",
    "    y = \"y_pred_trafo_avg\",\n",
    "    data = all_valid_results,\n",
    "    ax = ax2)\n",
    "sns.stripplot(x = \"unfavorable\",\n",
    "    y = \"y_pred_trafo_avg\",\n",
    "    color = 'blue',\n",
    "    data = all_valid_results,\n",
    "    ax = ax2)\n",
    "ax2.axhline(y = threshold[index], color = \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "all_results[\"threshold2\"] = valid_threshold\n",
    "all_results[\"y_pred_class2\"] = (all_results[\"y_pred_trafo_avg\"] >= all_results[\"threshold2\"]).astype(int)\n",
    "\n",
    "y_org = all_results[\"unfavorable\"]\n",
    "y_pred = all_results[\"y_pred_class2\"]\n",
    "y_pred_prob = all_results[\"y_pred_trafo_avg\"]\n",
    "\n",
    "# calculate fpr and tpr for probabilities\n",
    "fpr_prob, tpr_prob, thresholds_prob = metrics.roc_curve(y_org, y_pred_prob)\n",
    "\n",
    "# calculate fpr, tpr and thresholds for class\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_org, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# calculate AUC confint\n",
    "AUC0_CI = fm.compute_auc_ci(y_org, y_pred_prob)\n",
    "\n",
    "# calculate spez and sens confint\n",
    "cm = confusion_matrix(y_org, y_pred)\n",
    "# sens\n",
    "nobs = sum([cm[1,0],cm[1,1]])\n",
    "count = sum([cm[1,1]])\n",
    "sens_ci_low, sens_ci_upp = proportion_confint(count , nobs,  alpha=0.05, method='wilson')\n",
    "#spec \n",
    "nobs = sum([cm[0,1],cm[0,0]])\n",
    "count = sum([cm[0,0]])\n",
    "spec_ci_low, spec_ci_upp = proportion_confint(count , nobs,  alpha=0.05, method='wilson')\n",
    "\n",
    "# Calc Acc\n",
    "print(\"Accuracy to beat:     \" + str(round(1 - np.mean(y_org), 4)))\n",
    "print(\"Accuracy:             \" + str(round(np.mean(y_pred == y_org), 4)))\n",
    "print(\"-----------------[95% Conf.]-----------------\")\n",
    "\n",
    "print(\"AUC of probabilities: \" + str(round(metrics.auc(fpr_prob, tpr_prob), 4)) + \" \" +\n",
    "                                 str(np.around([AUC0_CI[0], AUC0_CI[1]],4)))\n",
    "print(\"NLL :                 \" + str(round(metrics.log_loss(y_org, y_pred_prob), 4)))\n",
    "\n",
    "print(\"Threshold:            \" + str(round(valid_threshold, 4)))\n",
    "\n",
    "print(\"Spezifität:           \" + str(round(1-fpr[1], 4)) + \" \" +\n",
    "                                 str(np.around([spec_ci_low, spec_ci_upp],4)))\n",
    "print(\"Sensitivität:         \" + str(round(tpr[1], 4)) + \" \" +\n",
    "                                 str(np.around([sens_ci_low, sens_ci_upp],4)))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10,5))\n",
    "\n",
    "# method I: plt\n",
    "ax1.title.set_text('Receiver Operating Characteristic')\n",
    "ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "ax1.legend(loc = 'lower right')\n",
    "ax1.plot([0, 1], [0, 1],'r--')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "\n",
    "sns.boxplot(x = \"unfavorable\",\n",
    "    y = \"y_pred_trafo_avg\",\n",
    "    data = all_results,\n",
    "    ax = ax2)\n",
    "sns.stripplot(x = \"unfavorable\",\n",
    "    y = \"y_pred_trafo_avg\",\n",
    "    hue = 'y_pred_class2',\n",
    "    palette=[\"C2\", \"C3\"],\n",
    "    data = all_results,\n",
    "    ax = ax2)\n",
    "plt.legend(title='predicted class', loc='upper center')\n",
    "ax2.set(xlabel='true class', ylabel='predicted probability (class 1)')\n",
    "ax2.axhline(y = valid_threshold, color = \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[all_results.p_id.isin([27,35,125,297,299,319,460,481,483,529])\n",
    "            ][[\"p_id\", \"mrs\", \"unfavorable\", \"test_split\", \n",
    "               \"y_pred_model_\" + \"0\", \"y_pred_model_\" + \"1\", \n",
    "               \"y_pred_model_\" + \"2\", \"y_pred_model_\" + \"3\", \n",
    "               \"y_pred_model_\" + \"4\", \n",
    "               \"y_pred_trafo_avg\", \"y_pred_class\"]].sort_values(by = \"p_id\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Model Uncertainty\n",
    "\n",
    "Use the standard deviation of the predictions as a measure of uncertainty. Then use min max normalization to scale the uncertainty between 0 and 1.  \n",
    "Compare the uncertainty with the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"y_pred_std\"] = all_results[[\"y_pred_model_\" + str(i) for i in range(5)]].std(axis = 1)\n",
    "all_results[\"y_pred_unc\"] = (all_results[\"y_pred_std\"] - all_results.y_pred_std.min()) / (\n",
    "    all_results.y_pred_std.max() - all_results.y_pred_std.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot uncertainty\n",
    "plt.figure(figsize = (4.6,5))\n",
    "sns.boxplot(x = \"unfavorable\",\n",
    "    y = \"y_pred_unc\",\n",
    "    data = all_results)\n",
    "g = sns.stripplot(x = \"unfavorable\",\n",
    "    y = \"y_pred_unc\",\n",
    "    hue = 'y_pred_class2',\n",
    "    alpha = 0.75,\n",
    "    palette=[\"C2\", \"C3\"],\n",
    "    data = all_results)\n",
    "plt.legend(title='predicted class', loc='upper center')\n",
    "g.set(xlabel='true class', ylabel='prediction uncertainty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"pred_correct\"] = all_results[\"y_pred_class2\"] == all_results[\"unfavorable\"] \n",
    "\n",
    "sns.boxplot(x = \"pred_correct\",\n",
    "    y = \"y_pred_unc\",\n",
    "    data = all_results)\n",
    "sns.stripplot(x = \"pred_correct\",\n",
    "    y = \"y_pred_unc\",\n",
    "    hue = 'unfavorable',\n",
    "    palette=[\"C2\", \"C3\"],\n",
    "    data = all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "           x = \"y_pred_trafo_avg\",\n",
    "           y = \"y_pred_unc\",\n",
    "           hue = \"unfavorable\",\n",
    "           data = all_results)\n",
    "plt.axvline(x = valid_threshold, color = \"red\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results\n",
    "\n",
    "Delete threshold 1 because mulitple different thresholds does not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = all_results.drop(columns = [\"y_pred_class\", \"threshold\"])\n",
    "all_results = all_results.rename(columns = {\"y_pred_class2\": \"y_pred_class\", \"threshold2\": \"threshold\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_file:\n",
    "    all_results.to_csv(DATA_OUTPUT_DIR + all_result_name + \".csv\",  index=False) # rename output file!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_result_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Splits\n",
    "\n",
    "Only used for andreas/paper split:  \n",
    "Shows the different predictions (and uncertainties) for patients which were in multiple test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if version == \"andrea\":    \n",
    "    andrea_splits = pd.read_csv(split_path, \n",
    "                                    sep='\\,', header = None, engine = 'python', \n",
    "                                    usecols = [1,2,3]).apply(lambda x: x.str.replace(r\"\\\"\",\"\"))\n",
    "    andrea_splits.columns = andrea_splits.iloc[0]\n",
    "    andrea_splits.drop(index=0, inplace=True)\n",
    "    andrea_splits = andrea_splits.astype({'idx': 'int32', 'spl': 'int32'})\n",
    "    print(andrea_splits[andrea_splits[\"type\"].isin([\"test\"])].idx.nunique())\n",
    "    print(sum(andrea_splits[\"type\"].isin([\"test\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if version == \"andrea\":\n",
    "    all_p_dup = all_results[all_results[\"p_id\"].duplicated()][\"p_id\"].unique()\n",
    "    all_dup = all_results[all_results[\"p_id\"].isin(all_p_dup)].sort_values(\"p_id\")\n",
    "    all_dup[\"p_id\"].value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if version == \"andrea\":\n",
    "    print(\"Total duplicate: \", len(all_dup[\"p_id\"].value_counts()))\n",
    "\n",
    "    print(\"gleich klassifieziert (%): \", np.mean(all_dup.groupby(\"p_id\").y_pred_class.mean().isin([0,1])))\n",
    "    print(\"gleich klassifieziert (abs): \", np.sum(all_dup.groupby(\"p_id\").y_pred_class.mean().isin([0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if version == \"andrea\":\n",
    "    sns.histplot(all_dup.groupby(\"p_id\").y_pred_trafo_avg.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if version == \"andrea\":\n",
    "    all_dup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Plot\n",
    "\n",
    "Plot the calibration of all models and the mean calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_plot_datas = []\n",
    "for split in which_splits:\n",
    "    dat = all_results.loc[all_results[\"test_split\"] == split]\n",
    "    cal_plot_datas.append(fm.cal_plot_data_prep(\n",
    "        dat[\"y_pred_trafo_avg\"].array, dat[\"unfavorable\"].array\n",
    "    ).select_dtypes(include=np.number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cal_plot_datas = pd.concat(cal_plot_datas)\n",
    "all_cal_plot_datas[\"iter\"] = all_cal_plot_datas.index\n",
    "all_cal_plot_datas = all_cal_plot_datas.groupby(\"iter\")[\n",
    "    [\"predicted_probability_middle\", \"observed_proportion\", \"observed_proportion_lower\", \"observed_proportion_upper\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(5, 5), dpi=80)\n",
    "\n",
    "for i in range(len(which_splits)):\n",
    "    fm.cal_plot(cal_plot_datas[i], \n",
    "             \"predicted_probability_middle\", \"observed_proportion\",\n",
    "                        \"observed_proportion_lower\", \"observed_proportion_upper\", alpha = .35, show = False)\n",
    "fm.cal_plot(all_cal_plot_datas, \"predicted_probability_middle\", \"observed_proportion\",\n",
    "                        \"observed_proportion_lower\", \"observed_proportion_upper\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Volume index\n",
    "\n",
    "Check if X_test is the same when selected via p_idx as when directly selected.  \n",
    "This must be true to make sure the correct volume is accessed for the heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"/tf/notebooks/hezo/stroke_zurich/data/dicom_2d_192x192x3_clean_interpolated_18_02_2021_preprocessed2.h5\", \"r\") as h5:\n",
    "# both images are the same\n",
    "    X_in = h5[\"X\"][:]\n",
    "    Y_img = h5[\"Y_img\"][:]\n",
    "    Y_pat = h5[\"Y_pat\"][:]\n",
    "    pat = h5[\"pat\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "results.p_id.array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(pat == results.p_id.array[index]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = X_test[index].astype(\"float64\")\n",
    "im2 = X_in[np.argwhere(pat == results.p_id.array[index]).squeeze()].astype(\"float64\")\n",
    "np.allclose(im1, im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "ax1.imshow(im1[:,:,14])\n",
    "ax2.imshow(im2[:,:,14])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
