{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "/tf/notebooks/schnemau/xAI_stroke_3d\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "#import functions_read_data as rdat\n",
    "# Tensorflow/Keras\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "\n",
    "DIR = \"/tf/notebooks/schnemau/xAI_stroke_3d/\"\n",
    "os.chdir(DIR)\n",
    "print(os.getcwd())\n",
    "IMG_DIR = \"/tf/notebooks/hezo/stroke_perfusion/data/\"\n",
    "OUTPUT_DIR = \"/tf/notebooks/schnemau/xAI_stroke_3d/data/\"\n",
    "path_img = IMG_DIR + 'dicom_2d_192x192x3_clean_interpolated_18_02_2021_preprocessed2.h5'\n",
    "path_tab = IMG_DIR + 'baseline_data_zurich_prepared.csv'\n",
    "\n",
    "\n",
    "# Own functions\n",
    "from functions.plot_slices import plot_slices\n",
    "# ontram functions\n",
    "from k_ontram_functions.ontram import ontram\n",
    "from k_ontram_functions.ontram_loss import ontram_loss\n",
    "from k_ontram_functions.ontram_metrics import ontram_acc, ontram_auc\n",
    "from k_ontram_functions.ontram_predict import predict_ontram, get_parameters\n",
    "from functions.augmentation3d import zoom, rotate, flip, shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_split = 6\n",
    "(X_train, X_valid, X_test, X_tab_train, X_tab_valid, X_tab_test), (Y_train, Y_valid, Y_test), results = read_and_split_img_data_andrea_maurice(\n",
    "    path_img = IMG_DIR + 'dicom_2d_192x192x3_clean_interpolated_18_02_2021_preprocessed2.h5', \n",
    "    path_tab = IMG_DIR + 'baseline_data_zurich_prepared.csv', \n",
    "    path_splits = '/tf/notebooks/schnemau/xAI_stroke_3d/data/andrea_splits.csv', \n",
    "    split = which_split)\n",
    "\n",
    "Y_train_MRS = to_categorical(Y_train)\n",
    "Y_valid_MRS = to_categorical(Y_valid)\n",
    "Y_test_MRS = to_categorical(Y_test)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3], 1))\n",
    "X_valid = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], X_valid.shape[2], X_valid.shape[3], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], X_test.shape[3], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = Y_train_MRS.shape[1]\n",
    "batch_size = 5\n",
    "epochs = 10\n",
    "ntrain = 300\n",
    "nvalid = 40\n",
    "ntest = 40\n",
    "\n",
    "# function for preprocessing\n",
    "def train_preprocessing(data, label):\n",
    "    \"\"\"Process training data.\"\"\"\n",
    "    volume = data\n",
    "    volume = zoom(volume)\n",
    "    volume = rotate(volume)\n",
    "    volume = shift(volume)\n",
    "    volume = flip(volume)\n",
    "    return volume, label\n",
    "\n",
    "# Define data sets for Jonas Model - no ordinal structure of 0,1\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((X_train[:ntrain], Y_train[:ntrain]))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((X_valid[:nvalid], Y_valid[:nvalid]))\n",
    "test_loader = tf.data.Dataset.from_tensor_slices((X_test[:ntest], Y_test[:ntest]))\n",
    "\n",
    "train_dataset = (train_loader.shuffle(len(X_train[:ntrain]))\n",
    "                 .map(train_preprocessing)\n",
    "                 .batch(batch_size, drop_remainder = True))\n",
    "\n",
    "validation_dataset = (validation_loader.batch(batch_size, drop_remainder = True))\n",
    "test_dataset = (test_loader.batch(len(X_test[:ntest])))\n",
    "\n",
    "# Define data sets MRS for ordinal structure\n",
    "train_loader_MRS = tf.data.Dataset.from_tensor_slices((X_train[:ntrain], Y_train_MRS[:ntrain]))\n",
    "validation_loader_MRS = tf.data.Dataset.from_tensor_slices((X_valid[:nvalid], Y_valid_MRS[:nvalid]))\n",
    "test_loader_MRS = tf.data.Dataset.from_tensor_slices((X_test[:ntest], Y_test_MRS[:ntest]))\n",
    "\n",
    "train_dataset_MRS = (train_loader_MRS.shuffle(len(X_train[:ntrain]))\n",
    "                 .map(train_preprocessing)\n",
    "                 .batch(batch_size, drop_remainder = True))\n",
    "\n",
    "validation_dataset_MRS = (validation_loader_MRS.batch(batch_size, drop_remainder = True))\n",
    "test_dataset_MRS = (test_loader_MRS.batch(len(X_test[:ntest])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_model_linear(input_shape, output_shape, activation = \"linear\"):\n",
    "    initializer = keras.initializers.he_normal(seed = 2202)\n",
    "    in_ = keras.Input(shape = input_shape)\n",
    "    x = keras.layers.Convolution3D(32, kernel_size=(3, 3, 3), padding = 'same', activation = 'relu')(in_)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    x = keras.layers.Convolution3D(32, kernel_size=(3, 3, 3), padding = 'same', activation = 'relu')(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    x = keras.layers.Convolution3D(64, kernel_size=(3, 3, 3), padding = 'same', activation = 'relu')(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    x = keras.layers.Convolution3D(64, kernel_size=(3, 3, 3), padding = 'same', activation = 'relu')(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(128, activation = 'relu')(x)\n",
    "    x = keras.layers.Dropout(0.3)(x)\n",
    "    x = keras.layers.Dense(128, activation = 'relu')(x)\n",
    "    x = keras.layers.Dropout(0.3)(x)\n",
    "    out_ = keras.layers.Dense(output_shape, activation = activation, use_bias = False)(x) \n",
    "    nn_im = keras.Model(inputs = in_, outputs = out_)\n",
    "    return nn_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontram_1 = ontram(img_model_linear(X_train.shape[1:], 1))\n",
    "ontram_2 = ontram(img_model_linear(X_train.shape[1:], 1))\n",
    "ontram_3 = ontram(img_model_linear(X_train.shape[1:], 1))\n",
    "ontram_4 = ontram(img_model_linear(X_train.shape[1:], 1))\n",
    "ontram_5 = ontram(img_model_linear(X_train.shape[1:], 1))\n",
    "\n",
    "ontram_1.compile(optimizer=keras.optimizers.Adam(),\n",
    "                                    loss=ontram_loss(C, batch_size),\n",
    "                                    metrics=[ontram_acc(C, batch_size)])\n",
    "\n",
    "ontram_2.compile(optimizer=keras.optimizers.Adam(),\n",
    "                                    loss=ontram_loss(C, batch_size),\n",
    "                                    metrics=[ontram_acc(C, batch_size)])\n",
    "ontram_3.compile(optimizer=keras.optimizers.Adam(),\n",
    "                                    loss=ontram_loss(C, batch_size),\n",
    "                                    metrics=[ontram_acc(C, batch_size)])\n",
    "\n",
    "ontram_4.compile(optimizer=keras.optimizers.Adam(),\n",
    "                                    loss=ontram_loss(C, batch_size),\n",
    "                                    metrics=[ontram_acc(C, batch_size)])\n",
    "\n",
    "ontram_5.compile(optimizer=keras.optimizers.Adam(),\n",
    "                                    loss=ontram_loss(C, batch_size),\n",
    "                                    metrics=[ontram_acc(C, batch_size)])\n",
    "\n",
    "ontram_1.load_weights('/tf/notebooks/schnemau/xAI_stroke_3d/ensembling_results/model_ontram_CIB_0.h5')\n",
    "ontram_2.load_weights('/tf/notebooks/schnemau/xAI_stroke_3d/ensembling_results/model_ontram_CIB_1.h5')\n",
    "ontram_3.load_weights('/tf/notebooks/schnemau/xAI_stroke_3d/ensembling_results/model_ontram_CIB_2.h5')\n",
    "ontram_4.load_weights('/tf/notebooks/schnemau/xAI_stroke_3d/ensembling_results/model_ontram_CIB_3.h5')\n",
    "ontram_5.load_weights('/tf/notebooks/schnemau/xAI_stroke_3d/ensembling_results/model_ontram_CIB_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8441382 ],\n",
       "       [0.84616756],\n",
       "       [0.84103656],\n",
       "       [0.84914815],\n",
       "       [0.85094553],\n",
       "       [0.8498644 ],\n",
       "       [0.8382138 ],\n",
       "       [0.8537668 ],\n",
       "       [0.84773195],\n",
       "       [0.84653074],\n",
       "       [0.8450489 ],\n",
       "       [0.8390857 ],\n",
       "       [0.8416159 ],\n",
       "       [0.84795034],\n",
       "       [0.8507792 ],\n",
       "       [0.8489443 ],\n",
       "       [0.84187025],\n",
       "       [0.843887  ],\n",
       "       [0.8480637 ],\n",
       "       [0.8577904 ],\n",
       "       [0.8340716 ],\n",
       "       [0.85149276],\n",
       "       [0.85094327],\n",
       "       [0.83287317],\n",
       "       [0.8504164 ],\n",
       "       [0.8505758 ],\n",
       "       [0.8331619 ],\n",
       "       [0.84225655],\n",
       "       [0.85278124],\n",
       "       [0.83733946],\n",
       "       [0.8504125 ],\n",
       "       [0.8432271 ],\n",
       "       [0.85069674],\n",
       "       [0.843454  ],\n",
       "       [0.8407243 ],\n",
       "       [0.8540609 ],\n",
       "       [0.8404932 ],\n",
       "       [0.83605474],\n",
       "       [0.8449385 ],\n",
       "       [0.84095687],\n",
       "       [0.8454236 ]], dtype=float32)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(ontram_1.predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def inv_sigmoid(x):\n",
    "    return np.log(x / (1 - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 329 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f90237720d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f90a003cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "predictions1 = ontram_1.predict(X_test)\n",
    "predictions2 = ontram_2.predict(X_test)\n",
    "predictions3 = ontram_3.predict(X_test)\n",
    "predictions4 = ontram_4.predict(X_test)\n",
    "predictions5 = ontram_5.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8193037 ],\n",
       "       [0.8197383 ],\n",
       "       [0.8185487 ],\n",
       "       [0.8188418 ],\n",
       "       [0.8207771 ],\n",
       "       [0.82614714],\n",
       "       [0.8232849 ],\n",
       "       [0.8200314 ],\n",
       "       [0.8238802 ],\n",
       "       [0.8206554 ],\n",
       "       [0.8126985 ],\n",
       "       [0.8208447 ],\n",
       "       [0.8163442 ],\n",
       "       [0.8248391 ],\n",
       "       [0.8201125 ],\n",
       "       [0.8218048 ],\n",
       "       [0.8254992 ],\n",
       "       [0.82419246],\n",
       "       [0.8223725 ],\n",
       "       [0.8230573 ],\n",
       "       [0.8130855 ],\n",
       "       [0.82391715],\n",
       "       [0.81903386],\n",
       "       [0.81978595],\n",
       "       [0.828861  ],\n",
       "       [0.81848574],\n",
       "       [0.81952757],\n",
       "       [0.82667315],\n",
       "       [0.8267326 ],\n",
       "       [0.8230211 ],\n",
       "       [0.82241046],\n",
       "       [0.82126534],\n",
       "       [0.8211114 ],\n",
       "       [0.8286204 ],\n",
       "       [0.8311095 ],\n",
       "       [0.82293516],\n",
       "       [0.82783055],\n",
       "       [0.82295007],\n",
       "       [0.8305129 ],\n",
       "       [0.83465314],\n",
       "       [0.8230796 ]], dtype=float32)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_predictions = np.stack((predictions1, predictions2, predictions3, predictions4, predictions5), axis=-1)\n",
    "mean_predictions = np.mean(stacked_predictions, axis=-1)\n",
    "sigmoid(mean_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_l = []\n",
    "    for model_name in model_names:\n",
    "        cnn.load_weights(model_name)\n",
    "        \n",
    "        heatmap_prob_sum = np.zeros((volume.shape[0], volume.shape[1], volume.shape[2]), np.float32)\n",
    "        heatmap_occ_n = np.zeros((volume.shape[0], volume.shape[1], volume.shape[2]), np.float32)\n",
    "\n",
    "        # for n, (x, y, z, vol_float) in tqdm.tqdm(enumerate(iter_occlusion(volume, size = occlusion_size, stride = occlusion_stride))):\n",
    "        #     X = vol_float.reshape(1, volume.shape[0], volume.shape[1], volume.shape[2], 1)\n",
    "        #     out = model.predict(X)\n",
    "\n",
    "        #     heatmap_prob_sum[x:x + occlusion_size[0], y:y + occlusion_size[1], z:z + occlusion_size[2]] += out[0]\n",
    "        #     heatmap_occ_n[x:x + occlusion_size[0], y:y + occlusion_size[1], z:z + occlusion_size[2]] += 1\n",
    "\n",
    "        ## Faster Implementation\n",
    "        \n",
    "        ## Generate all possible occlusions\n",
    "        X = []\n",
    "        xyz = []\n",
    "        for n, (x, y, z, vol_float) in enumerate(iter_occlusion(\n",
    "                volume, size = occlusion_size, stride = occlusion_stride)):\n",
    "            X.append(vol_float.reshape(volume.shape[0], volume.shape[1], volume.shape[2], 1))\n",
    "            xyz.append((x,y,z))\n",
    "        \n",
    "        X = np.array(X)\n",
    "        out = cnn.predict(X) # do prediction for all occlusions at once \n",
    "        \n",
    "        ## Add predictions to heatmap and count number of predictions per voxel\n",
    "        for i in range(len(xyz)):\n",
    "            x,y,z = xyz[i]\n",
    "            heatmap_prob_sum[x:x + occlusion_size[0], y:y + occlusion_size[1], z:z + occlusion_size[2]] += out[i,0]\n",
    "            heatmap_occ_n[x:x + occlusion_size[0], y:y + occlusion_size[1], z:z + occlusion_size[2]] += 1\n",
    "\n",
    "        hm = heatmap_prob_sum / heatmap_occ_n # calculate average probability per voxel\n",
    "        \n",
    "        ## Get cutoff, invert heatmap if necessary and normalize\n",
    "        cut_off = res_tab[\"y_pred_model_\" + model_name[-4:-3]][0]\n",
    "    \n",
    "        if (res_tab[\"y_pred_class\"][0] == 0 and invert_hm == \"pred_class\" and not both_directions) or (\n",
    "            invert_hm == \"never\" and not both_directions): \n",
    "            hm[hm < cut_off] = cut_off\n",
    "        elif (res_tab[\"y_pred_class\"][0] == 1 and invert_hm == \"pred_class\" and not both_directions) or (\n",
    "            invert_hm == \"always\" and not both_directions):\n",
    "            hm[hm > cut_off] = cut_off\n",
    "        elif both_directions:\n",
    "            hm = hm - cut_off\n",
    "        \n",
    "        if normalize and not both_directions:\n",
    "            hm = ((hm - hm.min())/(hm.max()-hm.min()))\n",
    "        elif normalize and both_directions:\n",
    "            hm_min_max = [np.min(hm), np.max(hm)]\n",
    "            hm_abs_max = np.max(np.abs(hm_min_max))\n",
    "            hm = hm / hm_abs_max\n",
    "        \n",
    "        h_l.append(hm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to average, done wrongly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [get_parameters(ontram_1)['intercept'], \n",
    "          get_parameters(ontram_2)['intercept'], \n",
    "          get_parameters(ontram_3)['intercept'], \n",
    "          get_parameters(ontram_4)['intercept'], \n",
    "          get_parameters(ontram_5)['intercept']]\n",
    "\n",
    "new_model = []\n",
    "for index_layer in range(len(models[0])):\n",
    "    current_weights = []\n",
    "    for index_weights in range(len(models[0][index_layer])):\n",
    "\n",
    "        current_weights.append(sum([models[index_models][index_layer][index_weights]                            \n",
    "                                for index_models in range(len(models))])/len(models))\n",
    "    \n",
    "    new_model.append(current_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontram_ensemble = ontram(img_model_linear(X_train.shape[1:], 1))\n",
    "ontram_ensemble.compile(optimizer=keras.optimizers.Adam(),\n",
    "                                    loss=ontram_loss(C, batch_size),\n",
    "                                    metrics=[ontram_acc(C, batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, layer in enumerate(ontram_ensemble.mod_baseline.layers):\n",
    "    if len(new_model[index]) == 0:\n",
    "        weights = np.array([])\n",
    "    else: \n",
    "        weights = new_model[index]\n",
    "    \n",
    "    ontram_ensemble.mod_baseline.layers[index].set_weights(weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
