{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an X Fold CV Binary Model based on 3D Image Data\n",
    "\n",
    "The model is based on the following paper https://arxiv.org/abs/2206.13302. However, only binary classification is performed.  \n",
    "In order to have each patient once in the test set the data is not split in the same way as in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import random\n",
    "import pickle as pkl\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from scipy import ndimage\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and set path before loading modules\n",
    "print(os.getcwd())\n",
    "DIR = \"/tf/notebooks/brdd/xAI_stroke_3d/\"\n",
    "if os.getcwd() != DIR:\n",
    "    os.chdir(DIR)\n",
    "\n",
    "import functions_metrics as fm\n",
    "import functions_model_definition as md\n",
    "import functions_read_data as rdat\n",
    "\n",
    "print(\"TF  Version\",tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.experimental.get_memory_usage(\"GPU:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of all Runs\n",
    "\n",
    "Following an overview of all runs and their corresponding version name. Each version is saved in a folder with the same name. Before a new versionn is trained, the folders must be created manually.\n",
    "\n",
    "- 10Fold_sigmoid_V0 (old name: 10Fold_sigmoid): 10 stratifed (with outcome mrs > 2 or mrs <= 2) Folds trained with the last layer beeing activated with sigmoid (5 ensembles per split)\n",
    "- 10Fold_softmax_V0: same Folds as 10Fold_sigmoid but last layer activated with softmax (5 ensembles per split)\n",
    "- 10Fold_softmax_V1: new 10 Fold stratified (with mrs) and last layer activated with softmax (10 ensembles per split)\n",
    "- 10Fold_sigmoid_V1: same Folds as 10Fold_softmax_V1 and last layer activated with sigmoid (10 ensembles per split)\n",
    "- 10Fold_sigmoid_V2: 10 Fold binary stratified (mrs > or <= 2) other seed than V0, and last layer activated with sigmoid (5 ensembles per split)\n",
    "- 10Fold_sigmoid_V2f: same as 10Fold_sigmoid_V2 but with flatten Layer\n",
    "- 10Fold_signoid_V3: 10 Fold binary stratified (mrs > or <= 2) without TIA patients, other seed than V0 and V2 and last layer activated wih sigmoid (5 ensembles per split)\n",
    "\n",
    "10Fold_sigmoid_V0 was trained twice, therefore 2 model_versions exist (1 and 2), for all other versions only 1 model_version exists. Both model_versions are saved in the same folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data & Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path + output path:\n",
    "print(os.getcwd())\n",
    "DATA_DIR = DIR + \"data/\"\n",
    "OUTPUT_DIR = DIR + \"weights/10Fold_sigmoid_V0/\"\n",
    "# check model name in kfold loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path of the data and the model\n",
    "\n",
    "id_tab = pd.read_csv(DATA_DIR + \"10Fold_ids_V0.csv\", sep=\",\") # for V0\n",
    "# id_tab = pd.read_csv(DATA_DIR + \"10Fold_ids_V1.csv\", sep=\",\") # for V1\n",
    "# id_tab = pd.read_csv(DATA_DIR + \"10Fold_ids_V2.csv\", sep=\",\") # for V2 and V2f\n",
    "# id_tab = pd.read_csv(DATA_DIR + \"10Fold_ids_V3.csv\", sep=\",\") # for V3\n",
    "X = np.load(DATA_DIR + \"prepocessed_dicom_3d.npy\")\n",
    "\n",
    "model_version = 2 # define the model version\n",
    "\n",
    "print(id_tab.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters of the model\n",
    "\n",
    "train = False # if False then the weigths are loaded and only the output is calculated\n",
    "\n",
    "# Define Model\n",
    "layer_connection = \"globalAveragePooling\" # globalAveragePooling, flatten\n",
    "last_activation = \"sigmoid\" # sigmoid, softmax\n",
    "\n",
    "if last_activation == \"sigmoid\":\n",
    "    LOSS = \"binary_crossentropy\"\n",
    "elif last_activation == \"softmax\":\n",
    "    LOSS = tf.keras.losses.categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 10   # 10 for all models\n",
    "num_models = 5    # see overview of all models (above)\n",
    "\n",
    "batch_size = 6    # 6 for all models\n",
    "epochs = 250      # 250 for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = np.expand_dims(X, axis = -1).shape[1:]\n",
    "\n",
    "if last_activation == \"sigmoid\":\n",
    "    output_dim = 1\n",
    "elif last_activation == \"softmax\":\n",
    "    output_dim = 2\n",
    "\n",
    "# call model\n",
    "model_3d = md.stroke_binary_3d(input_dim = input_dim,\n",
    "                               output_dim = output_dim,\n",
    "                               layer_connection = layer_connection,\n",
    "                               last_activation = last_activation)\n",
    "\n",
    "model_3d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_preprocessing(volume, label):\n",
    "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_auc = []\n",
    "test_nll = []\n",
    "test_sens = []\n",
    "test_spez = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over splits (kFold)\n",
    "\n",
    "start0 = time.time()\n",
    "for i in range(num_splits):\n",
    "    start1 = time.time()\n",
    "    print(\"\\n\\n\\n\\n################################################################################\")\n",
    "    print(\"Split \" + str(i))\n",
    "    print(\"################################################################################\\n\\n\\n\\n\")\n",
    "    \n",
    "    (X_train, X_valid, X_test), (y_train, y_valid, y_test) = rdat.split_data(id_tab, X, i)\n",
    "    \n",
    "    if last_activation == \"softmax\":\n",
    "        y_train_enc = to_categorical(y_train)\n",
    "        y_valid_enc = to_categorical(y_valid)\n",
    "        y_test_enc = to_categorical(y_test)\n",
    "    else:\n",
    "        y_train_enc = y_train\n",
    "        y_valid_enc = y_valid\n",
    "        y_test_enc = y_test\n",
    "    \n",
    "    # loop over model instances (ensembling)\n",
    "    for j in range(num_models):\n",
    "        start2 = time.time()\n",
    "        print(\"\\n\\n#######################################################\")\n",
    "        print(\"Split \" + str(i) + \" Model \" + str(j))\n",
    "        print(\"#######################################################\\n\\n\")\n",
    "        \n",
    "        if layer_connection == \"globalAveragePooling\":\n",
    "            model_name = (\"3d_cnn_binary_model_split\" + str(i) + \n",
    "                          \"_unnormalized_avg_layer_paper_model_\" + last_activation + \"_activation_\" + str(model_version) + str(j) + \".h5\")\n",
    "        elif layer_connection == \"flatten\":\n",
    "            model_name = (\"3d_cnn_binary_model_split\" + str(i) + \n",
    "                          \"_unnormalized_flat_layer_paper_model_\" + last_activation + \"_activation_\" + str(model_version) + str(j) + \".h5\")\n",
    "\n",
    "        # call model\n",
    "        model_3d = md.stroke_binary_3d(input_dim = input_dim,\n",
    "                                       output_dim = output_dim,\n",
    "                                       layer_connection = layer_connection,\n",
    "                                       last_activation = last_activation)\n",
    "        \n",
    "        # Define data loaders.\n",
    "        train_loader = tf.data.Dataset.from_tensor_slices((X_train, y_train_enc))\n",
    "        validation_loader = tf.data.Dataset.from_tensor_slices((X_valid, y_valid_enc))\n",
    "\n",
    "        # data augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            zoom_range=0.15,\n",
    "            shear_range=0.15,\n",
    "            fill_mode=\"nearest\")\n",
    "        datagen.fit(X_train)\n",
    "\n",
    "        validation_dataset = (\n",
    "            validation_loader.shuffle(len(X_valid))\n",
    "            .map(validation_preprocessing)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(2)\n",
    "        )\n",
    "\n",
    "        #compile\n",
    "        model_3d.compile(\n",
    "            loss=LOSS,\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=5*1e-5),\n",
    "            metrics=[\"acc\", tf.keras.metrics.AUC(name = \"auc\")]\n",
    "        )\n",
    "\n",
    "        # Define callbacks.\n",
    "        # checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "        #     OUTPUT_DIR + model_name, \n",
    "        #     save_best_only=True\n",
    "        # )\n",
    "        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "            filepath = OUTPUT_DIR + model_name,\n",
    "            verbose = (1 if i == 0 and j == 0 else 0),\n",
    "            save_weights_only = True,\n",
    "            monitor = \"val_loss\", #'val_acc',\n",
    "            mode = 'min',\n",
    "            save_best_only = True)\n",
    "\n",
    "        early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=100, restore_best_weights=True)\n",
    "        \n",
    "        \n",
    "        # Train the model, doing validation at the end of each epoch\n",
    "        if train:\n",
    "            hist = model_3d.fit(\n",
    "                datagen.flow(X_train, y_train_enc, batch_size=batch_size, shuffle=True),\n",
    "                validation_data=validation_dataset,\n",
    "                epochs=epochs,\n",
    "                shuffle=True,\n",
    "                verbose=(1 if i == 0 and j == 0 else 0),\n",
    "                callbacks=[checkpoint_cb, early_stopping_cb]\n",
    "            ) \n",
    "            pkl.dump(hist.history, open(OUTPUT_DIR + \"hist_\" + model_name[:-2] + \"pkl\", \"wb\"), protocol=4)\n",
    "            histplt = hist.history\n",
    "            \n",
    "        if not train:\n",
    "            histplt = pkl.load(open(OUTPUT_DIR + \"hist_\" + model_name[:-2] + \"pkl\", \"rb\"))\n",
    "            \n",
    "        # plot training history\n",
    "        plt.figure(figsize = (30,10))\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.plot(histplt['loss'], label = \"loss\")\n",
    "        plt.plot(histplt['val_loss'],label = \"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.plot(histplt['acc'], label = \"acc\")\n",
    "        plt.plot(histplt['val_acc'],label = \"val_acc\")\n",
    "        plt.legend()\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.plot(histplt['auc'], label = \"auc\")\n",
    "        plt.plot(histplt['val_auc'],label = \"val_auc\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "            \n",
    "            \n",
    "        # Model evaluation\n",
    "        model_3d.load_weights(OUTPUT_DIR + model_name)\n",
    "\n",
    "        model_3d.evaluate(x=X_test, y=y_test_enc, verbose = 0)\n",
    "        \n",
    "        (AUC, NLL, sens, spec) = fm.bin_class_report(\n",
    "            X_test, \n",
    "            y_test_enc, \n",
    "            model = model_3d)\n",
    "        \n",
    "        if last_activation == \"sigmoid\":\n",
    "            y_pred = model_3d.predict(X_test)\n",
    "        elif last_activation == \"softmax\":\n",
    "            y_pred = model_3d.predict(X_test)[:,1]\n",
    "\n",
    "        fpr, tpr, threshold = metrics.roc_curve(y_test, (y_pred))\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        # method I: plt\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "        plt.legend(loc = 'lower right')\n",
    "        plt.plot([0, 1], [0, 1],'r--')\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.show()\n",
    "        \n",
    "        test_auc.append(AUC)\n",
    "        test_nll.append(NLL)\n",
    "        test_sens.append(sens)\n",
    "        test_spez.append(spec)\n",
    "        \n",
    "        end2 = time.time()\n",
    "        print(\" \")   \n",
    "        print(\"Duration of Training: \" + str(end2-start2))  \n",
    "        \n",
    "    end1 = time.time()\n",
    "    print(\" \")   \n",
    "    print(\"Duration of Split: \" + str(end1-start1))  \n",
    "        \n",
    "end0 = time.time()\n",
    "print(\" \")\n",
    "print(\"Duration of Everything: \" + str(end0-start0))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
