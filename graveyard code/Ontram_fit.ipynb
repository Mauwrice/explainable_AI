{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-in, prepare data & load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install seaborn\n",
    "#!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle as pkl\n",
    "\n",
    "#import functions_read_data as rdat\n",
    "# Tensorflow/Keras\n",
    "import tensorflow as tf\n",
    "import time\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pickle as pkl\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/notebooks/schnemau/xAI_stroke_3d\n"
     ]
    }
   ],
   "source": [
    "DIR = \"/tf/notebooks/schnemau/xAI_stroke_3d/\"\n",
    "os.chdir(DIR)\n",
    "print(os.getcwd())\n",
    "IMG_DIR = \"/tf/notebooks/hezo/stroke_perfusion/data/\"\n",
    "path_img = IMG_DIR + 'dicom_2d_192x192x3_clean_interpolated_18_02_2021_preprocessed2.h5'\n",
    "path_tab = IMG_DIR + 'baseline_data_zurich_prepared.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_metrics as fm\n",
    "import functions_model_definition as md\n",
    "import functions_read_data as rdat\n",
    "import Utils_maurice as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own functions\n",
    "from functions.plot_slices import plot_slices\n",
    "#ontram functions\n",
    "from k_ontram_functions.ontram import ontram\n",
    "from k_ontram_functions.ontram_loss import ontram_loss\n",
    "from k_ontram_functions.ontram_metrics import ontram_acc, ontram_auc\n",
    "from k_ontram_functions.ontram_predict import predict_ontram, get_parameters\n",
    "from functions.augmentation3d import zoom, rotate, flip, shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_split = 6\n",
    "(X_train, X_valid, X_test, X_tab_train, X_tab_valid, X_tab_test), (Y_train, Y_valid, Y_test), results = utils.read_and_split_img_data_andrea_maurice(\n",
    "    path_img = IMG_DIR + 'dicom_2d_192x192x3_clean_interpolated_18_02_2021_preprocessed2.h5', \n",
    "    path_tab = IMG_DIR + 'baseline_data_zurich_prepared.csv', \n",
    "    path_splits = '/tf/notebooks/schnemau/xAI_stroke_3d/data/andrea_splits.csv', \n",
    "    split = which_split)\n",
    "\n",
    "Y_train_MRS = to_categorical(Y_train)\n",
    "Y_valid_MRS = to_categorical(Y_valid)\n",
    "Y_test_MRS = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = Y_train_MRS.shape[1]\n",
    "batch_size = 5\n",
    "num_epochs = 250\n",
    "num_steps_per_epoch = 1000\n",
    "ntrain = 325\n",
    "nvalid = 40\n",
    "ntest = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Intercept Image Only CI_B Ontram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CIB_noreal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_CIB_noreal:\n",
    "    for i in range(5):\n",
    "        # call model\n",
    "        input_dim = np.expand_dims(X_train, axis = -1).shape[1:]\n",
    "        output_dim = C-1\n",
    "        model_3d = ontram(utils.img_model_linear_final(input_dim, output_dim))         \n",
    "\n",
    "        # Define data loaders.\n",
    "        train_loader = tf.data.Dataset.from_tensor_slices((X_train[:ntrain], Y_train_MRS[:ntrain]))\n",
    "        validation_loader = tf.data.Dataset.from_tensor_slices((X_valid[:nvalid], Y_valid_MRS[:nvalid]))\n",
    "\n",
    "        def validation_preprocessing(volume, label):\n",
    "            volume = tf.expand_dims(volume, axis=3)\n",
    "            return volume, label\n",
    "\n",
    "        # data augmentation \n",
    "        datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            zoom_range=0.15,\n",
    "            shear_range=0.15,\n",
    "            fill_mode=\"nearest\")\n",
    "        datagen.fit(X_train[:ntrain])\n",
    "\n",
    "        validation_dataset = (\n",
    "            validation_loader.shuffle(len(X_valid[:nvalid]))\n",
    "            .map(validation_preprocessing)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(2)\n",
    "        )\n",
    "\n",
    "        #compile\n",
    "        model_3d.compile(optimizer=keras.optimizers.Adam(learning_rate=5*1e-5),\n",
    "                                loss=ontram_loss(C, batch_size),\n",
    "                                metrics=[ontram_acc(C, batch_size)])\n",
    "        \n",
    "        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "        filepath = os.path.join('/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/', f'CIBB_{i}.h5'),\n",
    "        verbose = 1,\n",
    "        save_weights_only = True,\n",
    "        monitor = \"val_loss\", #'val_acc',\n",
    "        mode = 'min',\n",
    "        save_best_only = True)\n",
    "\n",
    "        early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, restore_best_weights=True)\n",
    "        \n",
    "        hist = model_3d.fit(\n",
    "            datagen.flow(X_train, Y_train_MRS, batch_size=batch_size, shuffle=True),\n",
    "            validation_data=validation_dataset,\n",
    "            epochs=num_epochs,\n",
    "            shuffle=True,\n",
    "            verbose=1,\n",
    "            #steps_per_epoch = num_steps_per_epoch,        \n",
    "            callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "        \n",
    "        # Save the training history as a pickle file\n",
    "        os.makedirs('/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/hist', exist_ok=True)\n",
    "        with open(os.path.join('/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/hist', f'CIBB_{i}.pkl'), 'wb') as file:\n",
    "            pkl.dump(hist.history, file, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Intercept Image Only CI_B Ontram different augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CIB_2_noreal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.5413 - acc: 0.8092\n",
      "Epoch 00001: val_loss improved from inf to 0.53170, saving model to /tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/CIB_0_2.h5\n",
      "65/65 [==============================] - 42s 644ms/step - loss: 0.5413 - acc: 0.8092 - val_loss: 0.5317 - val_acc: 0.8250\n",
      "Epoch 2/250\n",
      "55/65 [========================>.....] - ETA: 6s - loss: 0.5142 - acc: 0.8036"
     ]
    }
   ],
   "source": [
    "if train_CIB_2_noreal:\n",
    "    for i in range(5):\n",
    "\n",
    "        input_dim = np.expand_dims(X_train, axis = -1).shape[1:]\n",
    "        output_dim = Y_test_MRS.shape[1]-1\n",
    "        model_3d = ontram(utils.img_model_linear_final(input_dim, output_dim))     \n",
    "\n",
    "        model_3d.compile(optimizer=keras.optimizers.Adam(lr = 0.001),\n",
    "                                        loss=ontram_loss(C, batch_size),\n",
    "                                        metrics=[ontram_acc(C, batch_size)])\n",
    "\n",
    "\n",
    "        train_loader_MRS = tf.data.Dataset.from_tensor_slices((np.expand_dims(X_train, axis = -1)[:ntrain], Y_train_MRS[:ntrain]))\n",
    "        validation_loader_MRS = tf.data.Dataset.from_tensor_slices((np.expand_dims(X_valid, axis = -1)[:nvalid], Y_valid_MRS[:nvalid]))\n",
    "        test_loader_MRS = tf.data.Dataset.from_tensor_slices((np.expand_dims(X_test, axis = -1)[:ntest], Y_test_MRS[:ntest]))\n",
    "\n",
    "        def train_preprocessing(data, label):\n",
    "            #intercept = data[0]  # intercept\n",
    "            volume = data  # shift: image\n",
    "            volume = zoom(volume)\n",
    "            volume = rotate(volume)\n",
    "            volume = shift(volume)\n",
    "            volume = flip(volume)\n",
    "            return (volume), label\n",
    "\n",
    "\n",
    "        train_dataset_MRS = (train_loader_MRS.shuffle(len(X_train[:ntrain]))\n",
    "                            .map(train_preprocessing)\n",
    "                            .batch(batch_size, drop_remainder=True))\n",
    "\n",
    "        validation_dataset_MRS = validation_loader_MRS.batch(batch_size)             \n",
    "\n",
    "        \n",
    "        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "        filepath = os.path.join('/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/', f'CIB_{i}_2.h5'),\n",
    "        verbose = 1,\n",
    "        save_weights_only = True,\n",
    "        monitor = \"val_loss\", #'val_acc',\n",
    "        mode = 'min',\n",
    "        save_best_only = True)\n",
    "\n",
    "        early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=100, restore_best_weights=True)\n",
    "        \n",
    "        hist = model_3d.fit(\n",
    "            train_dataset_MRS,\n",
    "            validation_data=validation_dataset_MRS,\n",
    "            epochs=250,\n",
    "            shuffle=True,\n",
    "            verbose=1, \n",
    "            callbacks=[checkpoint_cb, early_stopping_cb])              \n",
    "        \n",
    "        # Save the training history as a pickle file\n",
    "        os.makedirs('/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/hist', exist_ok=True)\n",
    "        with open(os.path.join('/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/hist', f'CIB_{i}_2.pkl'), 'wb') as file:\n",
    "            pkl.dump(hist.history, file, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Intercept Image CI_B and linear shift in tabular data LS_X Ontram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CIBLSX_noreal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_CIBLSX_noreal:\n",
    "    for i in range(5):\n",
    "        # call model\n",
    "        input_dim = np.expand_dims(X_train, axis = -1).shape[1:]\n",
    "        output_dim = C-1\n",
    "        CI_B = utils.img_model_linear_final(input_dim, output_dim)\n",
    "        LS_X = utils.mod_linear_shift(X_tab_train.shape[1])\n",
    "        model_3d = ontram(CI_B, LS_X)     \n",
    "\n",
    "        # Define data loaders.\n",
    "        train_loader = tf.data.Dataset.from_tensor_slices((X_train[:ntrain], X_tab_train[:ntrain],Y_train_MRS[:ntrain]))\n",
    "        validation_loader = tf.data.Dataset.from_tensor_slices((X_valid[:nvalid], X_tab_valid[:nvalid], Y_valid_MRS[:nvalid]))\n",
    "\n",
    "        def validation_preprocessing(volume, tabular,label):\n",
    "            volume = tf.expand_dims(volume, axis=3)\n",
    "            return (volume, tabular), label\n",
    "\n",
    "        # data augmentation \n",
    "        datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            zoom_range=0.15,\n",
    "            shear_range=0.15,\n",
    "            fill_mode=\"nearest\")\n",
    "        datagen.fit(X_train[:ntrain])\n",
    "\n",
    "        validation_dataset = (\n",
    "            validation_loader.shuffle(len(X_valid[:nvalid]))\n",
    "            .map(validation_preprocessing)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(2)\n",
    "        )\n",
    "\n",
    "        #compile\n",
    "        model_3d.compile(optimizer=keras.optimizers.Adam(learning_rate=5*1e-5),\n",
    "                                loss=ontram_loss(C, batch_size),\n",
    "                                metrics=[ontram_acc(C, batch_size)])\n",
    "        \n",
    "        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "        filepath = os.path.join('/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/', f'CIB_LSX_{i}.h5'),\n",
    "        verbose = 1,\n",
    "        save_weights_only = True,\n",
    "        monitor = \"val_loss\", #'val_acc',\n",
    "        mode = 'min',\n",
    "        save_best_only = True)\n",
    "\n",
    "        early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=100, restore_best_weights=True)\n",
    "        \n",
    "        hist = model_3d.fit(\n",
    "            datagen.flow((X_train[:ntrain], X_tab_train[:ntrain]),Y_train_MRS[:ntrain], batch_size=batch_size, shuffle=True),\n",
    "            validation_data=validation_dataset,\n",
    "            epochs=250,\n",
    "            shuffle=True,\n",
    "            verbose=1,\n",
    "            steps_per_epoch = num_steps_per_epoch,        \n",
    "            callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "        \n",
    "        # Save the training history as a pickle file\n",
    "        os.makedirs('/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/hist', exist_ok=True)\n",
    "        with open(os.path.join('/tf/notebooks/schnemau/xAI_stroke_3d/data/results_real/hist', f'CIB_LSX_{i}.pkl'), 'wb') as file:\n",
    "            pkl.dump(hist.history, file, protocol=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
