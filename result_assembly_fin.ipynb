{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF  Version 2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "print(\"TF  Version\",tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and set path before loading modules\n",
    "INPUT_DIR = \"/tf/notebooks/schnemau/xAI_stroke_3d/\"\n",
    "OUTPUT_DIR = \"/tf/notebooks/schnemau/xAI_stroke_3d/\"\n",
    "if os.getcwd() != OUTPUT_DIR:\n",
    "    os.chdir(OUTPUT_DIR)\n",
    "    \n",
    "import functions_model_definition as md\n",
    "import functions_read_data as rdat\n",
    "import functions_slider as sl\n",
    "#weights tuning functions\n",
    "import ens_weights_tuning as w_tune\n",
    "\n",
    "#ontram functions\n",
    "from k_ontram_functions.ontram import ontram\n",
    "from k_ontram_functions.ontram_loss import ontram_loss\n",
    "from k_ontram_functions.ontram_metrics import ontram_acc, ontram_auc\n",
    "from k_ontram_functions.ontram_predict import predict_ontram, get_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Version\n",
    "#version = \"CIBLSX\" # one of:\n",
    "version = \"CIB\" # one of:\n",
    "\n",
    "# Define Model Version\n",
    "model_version = 2\n",
    "\n",
    "# Select naming convention (for CIBLSX model_version >= 3 should be False)\n",
    "comp_mode = False # if True: use old naming convention\n",
    "\n",
    "# define paths\n",
    "DATA_DIR, WEIGHT_DIR, DATA_OUTPUT_DIR, PIC_OUTPUT_DIR, pic_save_name = rdat.dir_setup(\n",
    "    INPUT_DIR, OUTPUT_DIR, version, model_version, \n",
    "    compatibility_mode=comp_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load images and ids\n",
    "(X_in, pat_ids, id_tab, all_results_tab, pat_orig_tab, pat_norm_tab, num_models) = rdat.version_setup(\n",
    "    DATA_DIR = DATA_DIR, \n",
    "    version = version, \n",
    "    model_version = model_version,\n",
    "    compatibility_mode=comp_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "(input_dim_img, output_dim, LOSS, layer_connection, last_activation) = md.model_setup(version)\n",
    "\n",
    "model_3d = md.model_init(\n",
    "    version = version, \n",
    "    output_dim = output_dim,\n",
    "    LOSS = LOSS,\n",
    "    layer_connection = layer_connection,\n",
    "    last_activation = last_activation,\n",
    "    C = 2,\n",
    "    learning_rate = 5*1e-5,\n",
    "    batch_size = 6,\n",
    "    input_dim = input_dim_img,\n",
    "    input_dim_tab = pat_norm_tab.drop(columns=[\"p_id\"]).shape[1] if \"LSX\" in version else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Name\n",
    "generate_model_name = md.set_generate_model_name(\n",
    "    model_version = model_version, \n",
    "    layer_connection = layer_connection, \n",
    "    last_activation = last_activation, \n",
    "    path = WEIGHT_DIR,\n",
    "    compatability_mode=comp_mode)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nrs = list(range(5)) #num of ensembles\n",
    "which_splits = list(range(0,10)) # 10 Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = [] \n",
    "valid_list = [] \n",
    "weights = []\n",
    "\n",
    "for which_split in which_splits:\n",
    "    data_split = rdat.split_data(id_tab, X_in, which_split, X_tab = pat_norm_tab)\n",
    "\n",
    "    X_valid = np.expand_dims(data_split[\"X\"][\"valid\"], axis=-1)\n",
    "    X_test = np.expand_dims(data_split[\"X\"][\"test\"], axis=-1)\n",
    "    Y_valid = to_categorical(data_split[\"y\"][\"valid\"])\n",
    "    Y_test = to_categorical(data_split[\"y\"][\"test\"])\n",
    "\n",
    "    if pat_norm_tab is not None:\n",
    "        X_tab_test = data_split[\"X_tab\"][\"test\"]    \n",
    "        X_tab_valid = data_split[\"X_tab\"][\"valid\"]\n",
    "        test_data = tf.data.Dataset.from_tensor_slices((X_test, X_tab_test))\n",
    "        valid_data = tf.data.Dataset.from_tensor_slices((X_valid, X_tab_valid))\n",
    "        shift_params = []\n",
    "    else:\n",
    "        X_tab_test = None\n",
    "        X_tab_valid = None\n",
    "        test_data = tf.data.Dataset.from_tensor_slices((X_test))\n",
    "        valid_data = tf.data.Dataset.from_tensor_slices((X_valid))\n",
    "        shift_params = None\n",
    "\n",
    "    test_labels = tf.data.Dataset.from_tensor_slices((Y_test))\n",
    "    test_loader = tf.data.Dataset.zip((test_data, test_labels))\n",
    "    test_dataset_pred = (test_loader.batch(len(X_test)))\n",
    "\n",
    "    valid_labels = tf.data.Dataset.from_tensor_slices((Y_valid))\n",
    "    valid_loader = tf.data.Dataset.zip((valid_data, valid_labels))\n",
    "    valid_dataset_pred = (valid_loader.batch(len(X_valid)))  \n",
    "\n",
    "    results = id_tab[id_tab[\"fold\" + str(which_split)] == \"test\"].copy()       \n",
    "    results[\"test_split\"] = which_split        \n",
    "        \n",
    "    validation_results = pd.DataFrame(\n",
    "            {\"test_split\": which_split,\n",
    "            \"unfavorable\": data_split[\"y\"][\"valid\"]})\n",
    "      \n",
    "    y_test_preds = []\n",
    "    y_valid_preds = []   \n",
    "    intercepts_test = []\n",
    "    intercepts_val = []\n",
    "    \n",
    "    for model_nr in model_nrs:\n",
    "        model_3d.load_weights(generate_model_name(which_split, model_nr))\n",
    "\n",
    "        #test\n",
    "        predic = predict_ontram(model_3d, data = test_dataset_pred)['pdf'][:,1]\n",
    "        y_test_preds.append(predic.squeeze())\n",
    "        results[\"y_pred_model_\" + str(model_nr)] = y_test_preds[-1]\n",
    "\n",
    "        #valid\n",
    "        predicc = predict_ontram(model_3d, data = valid_dataset_pred)['pdf'][:,1]\n",
    "        y_valid_preds.append(predicc.squeeze())\n",
    "        validation_results[\"y_pred_model_\" + str(model_nr)] = y_valid_preds[-1]   \n",
    "\n",
    "        # Save Intercepts for tuning\n",
    "        #test\n",
    "        preds_test = model_3d.predict(test_dataset_pred)\n",
    "        intercepts_test.append(preds_test[:, 0])\n",
    "        #valid\n",
    "        preds_val = model_3d.predict(valid_dataset_pred)\n",
    "        intercepts_val.append(preds_val[:, 0])\n",
    "\n",
    "        # Save shift parameters if CIB_LSX\n",
    "        if shift_params is not None:\n",
    "            shift_params.append(get_parameters(model_3d)['shift'][0][0][0])       \n",
    "\n",
    "    y_test_preds = np.array(y_test_preds)\n",
    "    y_valid_preds = np.array(y_valid_preds) \n",
    "\n",
    "    weigths_tuned = w_tune.get_w(intercepts = intercepts_val,\n",
    "                             y_true = data_split[\"y\"][\"valid\"],\n",
    "                             shift = shift_params,\n",
    "                             X_tab = X_tab_valid)\n",
    "    \n",
    "    for model_nr in model_nrs:\n",
    "        results[f'weight_model_{model_nr}'] = weigths_tuned[model_nr]\n",
    "        validation_results[f'weight_model_{model_nr}'] = weigths_tuned[model_nr]\n",
    "\n",
    "    results[\"y_pred_trafo_avg\"] = w_tune.get_ensemble(intercepts = intercepts_test, \n",
    "                                                      shift = shift_params, \n",
    "                                                      X_tab = X_tab_test, \n",
    "                                                      weights=None)\n",
    "    \n",
    "    results[\"y_pred_trafo_avg_w\"] = w_tune.get_ensemble(intercepts = intercepts_test, \n",
    "                                                      shift = shift_params, \n",
    "                                                      X_tab = X_tab_test, \n",
    "                                                      weights=weigths_tuned)\n",
    "    \n",
    "    validation_results[\"y_pred_trafo_avg\"] = w_tune.get_ensemble(intercepts = intercepts_val, \n",
    "                                                      shift = shift_params, \n",
    "                                                      X_tab = X_tab_valid, \n",
    "                                                      weights=None) \n",
    "    \n",
    "    validation_results[\"y_pred_trafo_avg_w\"] = w_tune.get_ensemble(intercepts = intercepts_val, \n",
    "                                                      shift = shift_params, \n",
    "                                                      X_tab = X_tab_valid, \n",
    "                                                      weights=weigths_tuned)\n",
    "    \n",
    "    results_list.append(results)\n",
    "    valid_list.append(validation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Threshold\n",
    "\n",
    "Calculation of threshold for classification is done on validation data. Then applied to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_thresholds_avg = []\n",
    "valid_thresholds_avg_w = []\n",
    "\n",
    "for i, validation_results in enumerate(valid_list):\n",
    "  \n",
    "    y_org = validation_results[\"unfavorable\"]\n",
    "    y_pred_avg = validation_results[\"y_pred_trafo_avg\"]\n",
    "    y_pred_avg_w = validation_results[\"y_pred_trafo_avg_w\"]\n",
    "  \n",
    "    # calculate fpr, tpr and thresholds\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_org, y_pred_avg)\n",
    "    fpr_w, tpr_w, threshold_w = metrics.roc_curve(y_org, y_pred_avg_w)\n",
    "\n",
    "    #auc\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    roc_auc_w = metrics.auc(fpr_w, tpr_w)\n",
    "    \n",
    "    # calculate geometric mean of tpr and fpr to find best threshold\n",
    "    gmean = np.sqrt(tpr * (1 - fpr))\n",
    "    gmean_w = np.sqrt(tpr_w * (1 - fpr_w))\n",
    "\n",
    "    # Find the optimal threshold\n",
    "    index = np.argmax(gmean)\n",
    "    index_w = np.argmax(gmean_w)\n",
    "\n",
    "    valid_thresholds_avg.append(threshold[index])\n",
    "    valid_thresholds_avg_w.append(threshold_w[index_w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Threshold to Testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, results in enumerate(results_list):\n",
    "    results[\"threshold_avg\"] = valid_thresholds_avg[i]\n",
    "    results[\"threshold_avg_w\"] = valid_thresholds_avg_w[i]\n",
    "    \n",
    "    results[\"y_pred_class_avg\"] = (results[\"y_pred_trafo_avg\"] >= results[\"threshold_avg\"]).astype(int)\n",
    "    results[\"y_pred_class_avg_w\"] = (results[\"y_pred_trafo_avg_w\"] >= results[\"threshold_avg_w\"]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat all Pandas and Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat(results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Model Uncertainty\n",
    "\n",
    "Use the standard deviation of the predictions as a measure of uncertainty. Then use min max normalization to scale the uncertainty between 0 and 1.  \n",
    "Compare the uncertainty with the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"y_pred_std\"] = all_results[[\"y_pred_model_\" + str(i) for i in range(5)]].std(axis = 1)\n",
    "all_results[\"y_pred_unc\"] = (all_results[\"y_pred_std\"] - all_results.y_pred_std.min()) / (\n",
    "    all_results.y_pred_std.max() - all_results.y_pred_std.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_means = np.average(all_results[[\"y_pred_model_\" + str(i) for i in range(5)]],\n",
    "                            weights=all_results[[\"weight_model_\" + str(i) for i in range(5)]],\n",
    "                            axis=1)\n",
    "\n",
    "weighted_var = np.average((all_results[[\"y_pred_model_\" + str(i) for i in range(5)]] - weighted_means[:, np.newaxis])**2,\n",
    "                                         weights=all_results[[\"weight_model_\" + str(i) for i in range(5)]],\n",
    "                                         axis=1)\n",
    "\n",
    "all_results[\"y_pred_std_w\"] = np.sqrt(weighted_var)\n",
    "\n",
    "all_results[\"y_pred_unc_w\"] = (all_results[\"y_pred_std_w\"] - all_results[\"y_pred_std_w\"].min()) / (\n",
    "        all_results[\"y_pred_std_w\"].max() - all_results[\"y_pred_std_w\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"pred_correct\"] = all_results[\"y_pred_class_avg\"] == all_results[\"unfavorable\"] \n",
    "all_results[\"pred_correct_w\"] = all_results[\"y_pred_class_avg_w\"] == all_results[\"unfavorable\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not comp_mode:\n",
    "    res_name = DATA_DIR + \"all_tab_results_\" + version + \"_M\" + str(model_version) + \".csv\" # 10 Fold\n",
    "\n",
    "elif comp_mode:\n",
    "    res_name = DATA_DIR + \"all_tab_results_10Fold_\" + version + \"_M\" + str(model_version) + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/notebooks/schnemau/xAI_stroke_3d/data/all_tab_results_CIB_M2.csv'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.to_csv(res_name,  index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
